{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('D:/houseRent/train.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 81 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Id             1460 non-null   int64  \n",
      " 1   MSSubClass     1460 non-null   int64  \n",
      " 2   MSZoning       1460 non-null   object \n",
      " 3   LotFrontage    1201 non-null   float64\n",
      " 4   LotArea        1460 non-null   int64  \n",
      " 5   Street         1460 non-null   object \n",
      " 6   Alley          91 non-null     object \n",
      " 7   LotShape       1460 non-null   object \n",
      " 8   LandContour    1460 non-null   object \n",
      " 9   Utilities      1460 non-null   object \n",
      " 10  LotConfig      1460 non-null   object \n",
      " 11  LandSlope      1460 non-null   object \n",
      " 12  Neighborhood   1460 non-null   object \n",
      " 13  Condition1     1460 non-null   object \n",
      " 14  Condition2     1460 non-null   object \n",
      " 15  BldgType       1460 non-null   object \n",
      " 16  HouseStyle     1460 non-null   object \n",
      " 17  OverallQual    1460 non-null   int64  \n",
      " 18  OverallCond    1460 non-null   int64  \n",
      " 19  YearBuilt      1460 non-null   int64  \n",
      " 20  YearRemodAdd   1460 non-null   int64  \n",
      " 21  RoofStyle      1460 non-null   object \n",
      " 22  RoofMatl       1460 non-null   object \n",
      " 23  Exterior1st    1460 non-null   object \n",
      " 24  Exterior2nd    1460 non-null   object \n",
      " 25  MasVnrType     1452 non-null   object \n",
      " 26  MasVnrArea     1452 non-null   float64\n",
      " 27  ExterQual      1460 non-null   object \n",
      " 28  ExterCond      1460 non-null   object \n",
      " 29  Foundation     1460 non-null   object \n",
      " 30  BsmtQual       1423 non-null   object \n",
      " 31  BsmtCond       1423 non-null   object \n",
      " 32  BsmtExposure   1422 non-null   object \n",
      " 33  BsmtFinType1   1423 non-null   object \n",
      " 34  BsmtFinSF1     1460 non-null   int64  \n",
      " 35  BsmtFinType2   1422 non-null   object \n",
      " 36  BsmtFinSF2     1460 non-null   int64  \n",
      " 37  BsmtUnfSF      1460 non-null   int64  \n",
      " 38  TotalBsmtSF    1460 non-null   int64  \n",
      " 39  Heating        1460 non-null   object \n",
      " 40  HeatingQC      1460 non-null   object \n",
      " 41  CentralAir     1460 non-null   object \n",
      " 42  Electrical     1459 non-null   object \n",
      " 43  1stFlrSF       1460 non-null   int64  \n",
      " 44  2ndFlrSF       1460 non-null   int64  \n",
      " 45  LowQualFinSF   1460 non-null   int64  \n",
      " 46  GrLivArea      1460 non-null   int64  \n",
      " 47  BsmtFullBath   1460 non-null   int64  \n",
      " 48  BsmtHalfBath   1460 non-null   int64  \n",
      " 49  FullBath       1460 non-null   int64  \n",
      " 50  HalfBath       1460 non-null   int64  \n",
      " 51  BedroomAbvGr   1460 non-null   int64  \n",
      " 52  KitchenAbvGr   1460 non-null   int64  \n",
      " 53  KitchenQual    1460 non-null   object \n",
      " 54  TotRmsAbvGrd   1460 non-null   int64  \n",
      " 55  Functional     1460 non-null   object \n",
      " 56  Fireplaces     1460 non-null   int64  \n",
      " 57  FireplaceQu    770 non-null    object \n",
      " 58  GarageType     1379 non-null   object \n",
      " 59  GarageYrBlt    1379 non-null   float64\n",
      " 60  GarageFinish   1379 non-null   object \n",
      " 61  GarageCars     1460 non-null   int64  \n",
      " 62  GarageArea     1460 non-null   int64  \n",
      " 63  GarageQual     1379 non-null   object \n",
      " 64  GarageCond     1379 non-null   object \n",
      " 65  PavedDrive     1460 non-null   object \n",
      " 66  WoodDeckSF     1460 non-null   int64  \n",
      " 67  OpenPorchSF    1460 non-null   int64  \n",
      " 68  EnclosedPorch  1460 non-null   int64  \n",
      " 69  3SsnPorch      1460 non-null   int64  \n",
      " 70  ScreenPorch    1460 non-null   int64  \n",
      " 71  PoolArea       1460 non-null   int64  \n",
      " 72  PoolQC         7 non-null      object \n",
      " 73  Fence          281 non-null    object \n",
      " 74  MiscFeature    54 non-null     object \n",
      " 75  MiscVal        1460 non-null   int64  \n",
      " 76  MoSold         1460 non-null   int64  \n",
      " 77  YrSold         1460 non-null   int64  \n",
      " 78  SaleType       1460 non-null   object \n",
      " 79  SaleCondition  1460 non-null   object \n",
      " 80  SalePrice      1460 non-null   int64  \n",
      "dtypes: float64(3), int64(35), object(43)\n",
      "memory usage: 924.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LotFrontage']=df['LotFrontage'].fillna(df['LotFrontage'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Alley'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BsmtCond']=df['BsmtCond'].fillna(df['BsmtCond'].mode()[0])\n",
    "df['BsmtQual']=df['BsmtQual'].fillna(df['BsmtQual'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FireplaceQu']=df['FireplaceQu'].fillna(df['FireplaceQu'].mode()[0])\n",
    "df['GarageType']=df['GarageType'].fillna(df['GarageType'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['GarageYrBlt'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GarageFinish']=df['GarageFinish'].fillna(df['GarageFinish'].mode()[0])\n",
    "df['GarageQual']=df['GarageQual'].fillna(df['GarageQual'].mode()[0])\n",
    "df['GarageCond']=df['GarageCond'].fillna(df['GarageCond'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['PoolQC','Fence','MiscFeature'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.drop(['Id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSSubClass       0\n",
       "MSZoning         0\n",
       "LotFrontage      0\n",
       "LotArea          0\n",
       "Street           0\n",
       "                ..\n",
       "MoSold           0\n",
       "YrSold           0\n",
       "SaleType         0\n",
       "SaleCondition    0\n",
       "SalePrice        0\n",
       "Length: 75, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['MasVnrType']=df['MasVnrType'].fillna(df['MasVnrType'].mode()[0])\n",
    "df['MasVnrArea']=df['MasVnrArea'].fillna(df['MasVnrArea'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BsmtExposure']=df['BsmtExposure'].fillna(df['BsmtExposure'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x24378b45e88>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['BsmtFinType2']=df['BsmtFinType2'].fillna(df['BsmtFinType2'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['MSZoning','Street','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood',\n",
    "         'Condition2','BldgType','Condition1','HouseStyle','SaleType',\n",
    "        'SaleCondition','ExterCond',\n",
    "         'ExterQual','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n",
    "        'RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Heating','HeatingQC',\n",
    "         'CentralAir',\n",
    "         'Electrical','KitchenQual','Functional',\n",
    "         'FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2437a2b45c8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_onehot_multcols(multcolumns):\n",
    "    df_final=final_df\n",
    "    i=0\n",
    "    for fields in multcolumns:\n",
    "        \n",
    "        print(fields)\n",
    "        df1=pd.get_dummies(final_df[fields],drop_first=True)\n",
    "        \n",
    "        final_df.drop([fields],axis=1,inplace=True)\n",
    "        if i==0:\n",
    "            df_final=df1.copy()\n",
    "        else:\n",
    "            \n",
    "            df_final=pd.concat([df_final,df1],axis=1)\n",
    "        i=i+1\n",
    "       \n",
    "        \n",
    "    df_final=pd.concat([final_df,df_final],axis=1)\n",
    "        \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=pd.read_csv('D:/houseRent/formulatedtest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=pd.concat([df,test_df],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       208500.0\n",
       "1       181500.0\n",
       "2       223500.0\n",
       "3       140000.0\n",
       "4       250000.0\n",
       "          ...   \n",
       "1454         NaN\n",
       "1455         NaN\n",
       "1456         NaN\n",
       "1457         NaN\n",
       "1458         NaN\n",
       "Name: SalePrice, Length: 2881, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSZoning\n",
      "Street\n",
      "LotShape\n",
      "LandContour\n",
      "Utilities\n",
      "LotConfig\n",
      "LandSlope\n",
      "Neighborhood\n",
      "Condition2\n",
      "BldgType\n",
      "Condition1\n",
      "HouseStyle\n",
      "SaleType\n",
      "SaleCondition\n",
      "ExterCond\n",
      "ExterQual\n",
      "Foundation\n",
      "BsmtQual\n",
      "BsmtCond\n",
      "BsmtExposure\n",
      "BsmtFinType1\n",
      "BsmtFinType2\n",
      "RoofStyle\n",
      "RoofMatl\n",
      "Exterior1st\n",
      "Exterior2nd\n",
      "MasVnrType\n",
      "Heating\n",
      "HeatingQC\n",
      "CentralAir\n",
      "Electrical\n",
      "KitchenQual\n",
      "Functional\n",
      "FireplaceQu\n",
      "GarageType\n",
      "GarageFinish\n",
      "GarageQual\n",
      "GarageCond\n",
      "PavedDrive\n"
     ]
    }
   ],
   "source": [
    "final_df=category_onehot_multcols(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df =final_df.loc[:,~final_df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>Min1</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>160</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1936</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>160</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1894</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>20</td>\n",
       "      <td>160.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1960</td>\n",
       "      <td>1996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1224.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>85</td>\n",
       "      <td>62.0</td>\n",
       "      <td>10441</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>60</td>\n",
       "      <td>74.0</td>\n",
       "      <td>9627</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1993</td>\n",
       "      <td>1994</td>\n",
       "      <td>94.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2881 rows Ã— 175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0             60         65.0     8450            7            5       2003   \n",
       "1             20         80.0     9600            6            8       1976   \n",
       "2             60         68.0    11250            7            5       2001   \n",
       "3             70         60.0     9550            7            5       1915   \n",
       "4             60         84.0    14260            8            5       2000   \n",
       "...          ...          ...      ...          ...          ...        ...   \n",
       "1454         160         21.0     1936            4            7       1970   \n",
       "1455         160         21.0     1894            4            5       1970   \n",
       "1456          20        160.0    20000            5            7       1960   \n",
       "1457          85         62.0    10441            5            5       1992   \n",
       "1458          60         74.0     9627            7            5       1993   \n",
       "\n",
       "      YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  Min1  Min2  Typ  \\\n",
       "0             2003       196.0       706.0         0.0  ...     0     0    1   \n",
       "1             1976         0.0       978.0         0.0  ...     0     0    1   \n",
       "2             2002       162.0       486.0         0.0  ...     0     0    1   \n",
       "3             1970         0.0       216.0         0.0  ...     0     0    1   \n",
       "4             2000       350.0       655.0         0.0  ...     0     0    1   \n",
       "...            ...         ...         ...         ...  ...   ...   ...  ...   \n",
       "1454          1970         0.0         0.0         0.0  ...     0     0    1   \n",
       "1455          1970         0.0       252.0         0.0  ...     0     0    1   \n",
       "1456          1996         0.0      1224.0         0.0  ...     0     0    1   \n",
       "1457          1992         0.0       337.0         0.0  ...     0     0    1   \n",
       "1458          1994        94.0       758.0         0.0  ...     0     0    1   \n",
       "\n",
       "      Attchd  Basment  BuiltIn  CarPort  Detchd  RFn  P  \n",
       "0          1        0        0        0       0    1  0  \n",
       "1          1        0        0        0       0    1  0  \n",
       "2          1        0        0        0       0    1  0  \n",
       "3          0        0        0        0       1    0  0  \n",
       "4          1        0        0        0       0    1  0  \n",
       "...      ...      ...      ...      ...     ...  ... ..  \n",
       "1454       1        0        0        0       0    0  0  \n",
       "1455       0        0        0        1       0    0  0  \n",
       "1456       0        0        0        0       1    0  0  \n",
       "1457       1        0        0        0       0    0  0  \n",
       "1458       1        0        0        0       0    0  0  \n",
       "\n",
       "[2881 rows x 175 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train=final_df.iloc[:1422,:]\n",
    "df_Test=final_df.iloc[1422:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda packages\\lib\\site-packages\\pandas\\core\\frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "df_Test.drop(['SalePrice'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df_Train.drop(['SalePrice'],axis=1)\n",
    "y_train=df_Train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import xgboost\n",
    "classifier=xgboost.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "regressor=xgboost.XGBRegressor()\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster=['gbtree','gblinear']\n",
    "base_score=[0.25,0.5,0.75,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [100, 500, 900, 1100, 1500]\n",
    "max_depth = [2, 3, 5, 10, 15]\n",
    "booster=['gbtree','gblinear']\n",
    "learning_rate=[0.05,0.1,0.15,0.20]\n",
    "min_child_weight=[1,2,3,4]\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "hyperparameter_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_depth':max_depth,\n",
    "    'learning_rate':learning_rate,\n",
    "    'min_child_weight':min_child_weight,\n",
    "    'booster':booster,\n",
    "    'base_score':base_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_cv = RandomizedSearchCV(estimator=regressor,\n",
    "            param_distributions=hyperparameter_grid,\n",
    "            cv=5, n_iter=50,\n",
    "            scoring = 'neg_mean_absolute_error',n_jobs = 4,\n",
    "            verbose = 5, \n",
    "            return_train_score = True,\n",
    "            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=4)]: Done 250 out of 250 | elapsed:  5.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:46:15] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score=nan,\n",
       "                   estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                          colsample_bylevel=1,\n",
       "                                          colsample_bynode=1,\n",
       "                                          colsample_bytree=1, gamma=0,\n",
       "                                          importance_type='gain',\n",
       "                                          learning_rate=0.1, max_delta_step=0,\n",
       "                                          max_depth=3, min_child_weight=1,\n",
       "                                          missing=None, n_estimators=100,\n",
       "                                          n_jobs=1, nthread=None,\n",
       "                                          objective='reg:linear',\n",
       "                                          random_state=0, reg_alpha=...\n",
       "                   iid='deprecated', n_iter=50, n_jobs=4,\n",
       "                   param_distributions={'base_score': [0.25, 0.5, 0.75, 1],\n",
       "                                        'booster': ['gbtree', 'gblinear'],\n",
       "                                        'learning_rate': [0.05, 0.1, 0.15, 0.2],\n",
       "                                        'max_depth': [2, 3, 5, 10, 15],\n",
       "                                        'min_child_weight': [1, 2, 3, 4],\n",
       "                                        'n_estimators': [100, 500, 900, 1100,\n",
       "                                                         1500]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=True, scoring='neg_mean_absolute_error',\n",
       "                   verbose=5)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=2, min_child_weight=1, missing=None, n_estimators=900,\n",
       "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=2, min_child_weight=1, missing=None, n_estimators=900,\n",
       "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "random_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=xgboost.XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "       max_depth=2, min_child_weight=1, missing=None, n_estimators=900,\n",
    "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=2, min_child_weight=1, missing=None, n_estimators=900,\n",
       "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=True, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'finalized_model.pkl'\n",
    "pickle.dump(classifier, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\anaconda packages\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda packages\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda packages\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda packages\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda packages\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda packages\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\anaconda packages\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda packages\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda packages\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda packages\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda packages\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda packages\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras import backend as K\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1459 entries, 0 to 1458\n",
      "Columns: 174 entries, MSSubClass to P\n",
      "dtypes: float64(10), int64(25), uint8(139)\n",
      "memory usage: 608.4 KB\n"
     ]
    }
   ],
   "source": [
    "df_Test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-56-668f88cbccb5>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-56-668f88cbccb5>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    y_pred=regressor.predict(df_Test,axis=1))\u001b[0m\n\u001b[1;37m                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "y_pred=regressor.predict(df_Test,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>Min1</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1961</td>\n",
       "      <td>1961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1958</td>\n",
       "      <td>1958</td>\n",
       "      <td>108.0</td>\n",
       "      <td>923.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1997</td>\n",
       "      <td>1998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>791.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "      <td>20.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 174 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          20         80.0    11622            5            6       1961   \n",
       "1          20         81.0    14267            6            6       1958   \n",
       "2          60         74.0    13830            5            5       1997   \n",
       "3          60         78.0     9978            6            6       1998   \n",
       "4         120         43.0     5005            8            5       1992   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  Min1  Min2  Typ  \\\n",
       "0          1961         0.0       468.0       144.0  ...     0     0    1   \n",
       "1          1958       108.0       923.0         0.0  ...     0     0    1   \n",
       "2          1998         0.0       791.0         0.0  ...     0     0    1   \n",
       "3          1998        20.0       602.0         0.0  ...     0     0    1   \n",
       "4          1992         0.0       263.0         0.0  ...     0     0    1   \n",
       "\n",
       "   Attchd  Basment  BuiltIn  CarPort  Detchd  RFn  P  \n",
       "0       1        0        0        0       0    0  0  \n",
       "1       1        0        0        0       0    0  0  \n",
       "2       1        0        0        0       0    0  0  \n",
       "3       1        0        0        0       0    0  0  \n",
       "4       1        0        0        0       0    1  0  \n",
       "\n",
       "[5 rows x 174 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=regressor.predict(df_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([117275.625, 163568.39 , 188306.14 , ..., 181178.69 , 115435.21 ,\n",
       "       236526.36 ], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=pd.DataFrame(y_pred)\n",
    "sub_df=pd.read_csv('D:/houseRent/sample_submission.csv')\n",
    "datasets=pd.concat([sub_df['Id'],pred],axis=1)\n",
    "datasets.columns=['Id','SalePrice']\n",
    "datasets.to_csv('D:/houseRent/sample_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.columns=['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df=df_Train['SalePrice'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.column=['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda packages\\lib\\site-packages\\pandas\\core\\frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "df_Train.drop(['SalePrice'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train=pd.concat([df_Train,temp_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>Min1</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1961</td>\n",
       "      <td>1961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1958</td>\n",
       "      <td>1958</td>\n",
       "      <td>108.0</td>\n",
       "      <td>923.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1997</td>\n",
       "      <td>1998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>791.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "      <td>20.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 174 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          20         80.0    11622            5            6       1961   \n",
       "1          20         81.0    14267            6            6       1958   \n",
       "2          60         74.0    13830            5            5       1997   \n",
       "3          60         78.0     9978            6            6       1998   \n",
       "4         120         43.0     5005            8            5       1992   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  Min1  Min2  Typ  \\\n",
       "0          1961         0.0       468.0       144.0  ...     0     0    1   \n",
       "1          1958       108.0       923.0         0.0  ...     0     0    1   \n",
       "2          1998         0.0       791.0         0.0  ...     0     0    1   \n",
       "3          1998        20.0       602.0         0.0  ...     0     0    1   \n",
       "4          1992         0.0       263.0         0.0  ...     0     0    1   \n",
       "\n",
       "   Attchd  Basment  BuiltIn  CarPort  Detchd  RFn  P  \n",
       "0       1        0        0        0       0    0  0  \n",
       "1       1        0        0        0       0    0  0  \n",
       "2       1        0        0        0       0    0  0  \n",
       "3       1        0        0        0       0    0  0  \n",
       "4       1        0        0        0       0    1  0  \n",
       "\n",
       "[5 rows x 174 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Test=pd.concat([df_Test,pred],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train=pd.concat([df_Train,df_Test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df_Train.drop(['SalePrice'],axis=1)\n",
    "y_train=df_Train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda packages\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=174, units=50, kernel_initializer=\"he_uniform\")`\n",
      "  if sys.path[0] == '':\n",
      "D:\\anaconda packages\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=25, kernel_initializer=\"he_uniform\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "D:\\anaconda packages\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=50, kernel_initializer=\"he_uniform\")`\n",
      "D:\\anaconda packages\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1, kernel_initializer=\"he_uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda packages\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda packages\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda packages\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 2304 samples, validate on 577 samples\n",
      "Epoch 1/1000\n",
      "2304/2304 [==============================] - 1s 607us/step - loss: 119919.2246 - val_loss: 53602.3849\n",
      "Epoch 2/1000\n",
      "2304/2304 [==============================] - 1s 342us/step - loss: 61640.9627 - val_loss: 49640.7828\n",
      "Epoch 3/1000\n",
      "2304/2304 [==============================] - 1s 282us/step - loss: 57642.7273 - val_loss: 45402.2469\n",
      "Epoch 4/1000\n",
      "2304/2304 [==============================] - 1s 282us/step - loss: 52261.1796 - val_loss: 40810.3519\n",
      "Epoch 5/1000\n",
      "2304/2304 [==============================] - 1s 267us/step - loss: 47244.8796 - val_loss: 37285.7483\n",
      "Epoch 6/1000\n",
      "2304/2304 [==============================] - 1s 252us/step - loss: 43240.5932 - val_loss: 33445.8947\n",
      "Epoch 7/1000\n",
      "2304/2304 [==============================] - 1s 260us/step - loss: 39902.8216 - val_loss: 32467.8830\n",
      "Epoch 8/1000\n",
      "2304/2304 [==============================] - 1s 236us/step - loss: 38495.1377 - val_loss: 31591.6703\n",
      "Epoch 9/1000\n",
      "2304/2304 [==============================] - 1s 319us/step - loss: 37633.8925 - val_loss: 32315.7839\n",
      "Epoch 10/1000\n",
      "2304/2304 [==============================] - 1s 337us/step - loss: 36761.8271 - val_loss: 32061.9453\n",
      "Epoch 11/1000\n",
      "2304/2304 [==============================] - 1s 283us/step - loss: 36413.4374 - val_loss: 31614.9538\n",
      "Epoch 12/1000\n",
      "2304/2304 [==============================] - 1s 269us/step - loss: 36203.1070 - val_loss: 31685.7165\n",
      "Epoch 13/1000\n",
      "2304/2304 [==============================] - 1s 255us/step - loss: 35885.7067 - val_loss: 31799.7521\n",
      "Epoch 14/1000\n",
      "2304/2304 [==============================] - 0s 208us/step - loss: 35981.6223 - val_loss: 31840.6952\n",
      "Epoch 15/1000\n",
      "2304/2304 [==============================] - 1s 245us/step - loss: 35275.4497 - val_loss: 31411.3485\n",
      "Epoch 16/1000\n",
      "2304/2304 [==============================] - 1s 290us/step - loss: 35378.6853 - val_loss: 31707.5581\n",
      "Epoch 17/1000\n",
      "2304/2304 [==============================] - 1s 283us/step - loss: 35374.4316 - val_loss: 31546.3621\n",
      "Epoch 18/1000\n",
      "2304/2304 [==============================] - 1s 281us/step - loss: 34752.4283 - val_loss: 31039.0634\n",
      "Epoch 19/1000\n",
      "2304/2304 [==============================] - 1s 290us/step - loss: 34502.5801 - val_loss: 32004.8486\n",
      "Epoch 20/1000\n",
      "2304/2304 [==============================] - 1s 290us/step - loss: 34280.2399 - val_loss: 31785.4135\n",
      "Epoch 21/1000\n",
      "2304/2304 [==============================] - 1s 285us/step - loss: 34127.7348 - val_loss: 31069.9328\n",
      "Epoch 22/1000\n",
      "2304/2304 [==============================] - 1s 310us/step - loss: 34002.2918 - val_loss: 30799.4119\n",
      "Epoch 23/1000\n",
      "2304/2304 [==============================] - 1s 329us/step - loss: 33686.3039 - val_loss: 30627.9892\n",
      "Epoch 24/1000\n",
      "2304/2304 [==============================] - 1s 341us/step - loss: 33711.9053 - val_loss: 30530.8793\n",
      "Epoch 25/1000\n",
      "2304/2304 [==============================] - 1s 246us/step - loss: 33458.5135 - val_loss: 30686.4846\n",
      "Epoch 26/1000\n",
      "2304/2304 [==============================] - 1s 235us/step - loss: 33446.1487 - val_loss: 30090.8028\n",
      "Epoch 27/1000\n",
      "2304/2304 [==============================] - 1s 234us/step - loss: 33512.9699 - val_loss: 31054.2632\n",
      "Epoch 28/1000\n",
      "2304/2304 [==============================] - 1s 237us/step - loss: 32985.8043 - val_loss: 30658.2733\n",
      "Epoch 29/1000\n",
      "2304/2304 [==============================] - 1s 238us/step - loss: 33014.5226 - val_loss: 29865.2763\n",
      "Epoch 30/1000\n",
      "2304/2304 [==============================] - 1s 224us/step - loss: 33294.0403 - val_loss: 30058.3903\n",
      "Epoch 31/1000\n",
      "2304/2304 [==============================] - 1s 248us/step - loss: 32808.8618 - val_loss: 30255.0877\n",
      "Epoch 32/1000\n",
      "2304/2304 [==============================] - 1s 236us/step - loss: 32865.0409 - val_loss: 29590.1359\n",
      "Epoch 33/1000\n",
      "2304/2304 [==============================] - 1s 230us/step - loss: 32930.9874 - val_loss: 29762.4481\n",
      "Epoch 34/1000\n",
      "2304/2304 [==============================] - 1s 317us/step - loss: 32397.4383 - val_loss: 30305.6112\n",
      "Epoch 35/1000\n",
      "2304/2304 [==============================] - 1s 298us/step - loss: 32337.3816 - val_loss: 29692.7183\n",
      "Epoch 36/1000\n",
      "2304/2304 [==============================] - 1s 306us/step - loss: 32426.3231 - val_loss: 29286.4655\n",
      "Epoch 37/1000\n",
      "2304/2304 [==============================] - 1s 287us/step - loss: 32226.1152 - val_loss: 29546.3629\n",
      "Epoch 38/1000\n",
      "2304/2304 [==============================] - 1s 247us/step - loss: 32467.4237 - val_loss: 29136.1326\n",
      "Epoch 39/1000\n",
      "2304/2304 [==============================] - 1s 332us/step - loss: 32314.2557 - val_loss: 29850.5307\n",
      "Epoch 40/1000\n",
      "2304/2304 [==============================] - 1s 328us/step - loss: 32118.1757 - val_loss: 29148.8012\n",
      "Epoch 41/1000\n",
      "2304/2304 [==============================] - 1s 300us/step - loss: 32262.4138 - val_loss: 30030.4682\n",
      "Epoch 42/1000\n",
      "2304/2304 [==============================] - 1s 306us/step - loss: 31949.2842 - val_loss: 29648.3041\n",
      "Epoch 43/1000\n",
      "2304/2304 [==============================] - 1s 301us/step - loss: 31702.2545 - val_loss: 28842.8656\n",
      "Epoch 44/1000\n",
      "2304/2304 [==============================] - 1s 274us/step - loss: 31757.9072 - val_loss: 29106.7303\n",
      "Epoch 45/1000\n",
      "2304/2304 [==============================] - 1s 252us/step - loss: 31640.4433 - val_loss: 29761.2221\n",
      "Epoch 46/1000\n",
      "2304/2304 [==============================] - 1s 259us/step - loss: 31368.7509 - val_loss: 29126.0909\n",
      "Epoch 47/1000\n",
      "2304/2304 [==============================] - 1s 242us/step - loss: 31575.7809 - val_loss: 28852.7457\n",
      "Epoch 48/1000\n",
      "2304/2304 [==============================] - 1s 320us/step - loss: 31437.1829 - val_loss: 29253.1037\n",
      "Epoch 49/1000\n",
      "2304/2304 [==============================] - 1s 327us/step - loss: 31362.0389 - val_loss: 28622.8563\n",
      "Epoch 50/1000\n",
      "2304/2304 [==============================] - 1s 327us/step - loss: 31381.1685 - val_loss: 28645.5200\n",
      "Epoch 51/1000\n",
      "2304/2304 [==============================] - 1s 299us/step - loss: 31402.7520 - val_loss: 28629.6304\n",
      "Epoch 52/1000\n",
      "2304/2304 [==============================] - 1s 309us/step - loss: 31312.4083 - val_loss: 28446.4752\n",
      "Epoch 53/1000\n",
      "2304/2304 [==============================] - 1s 297us/step - loss: 31040.8216 - val_loss: 28351.7159\n",
      "Epoch 54/1000\n",
      "2304/2304 [==============================] - 1s 297us/step - loss: 31575.7833 - val_loss: 28436.2800\n",
      "Epoch 55/1000\n",
      "2304/2304 [==============================] - 1s 302us/step - loss: 31271.5335 - val_loss: 28334.5467\n",
      "Epoch 56/1000\n",
      "2304/2304 [==============================] - 1s 249us/step - loss: 31185.6788 - val_loss: 28569.8833\n",
      "Epoch 57/1000\n",
      "2304/2304 [==============================] - 1s 235us/step - loss: 31175.8566 - val_loss: 28519.4130\n",
      "Epoch 58/1000\n",
      "2304/2304 [==============================] - 1s 330us/step - loss: 31007.9631 - val_loss: 29328.7013\n",
      "Epoch 59/1000\n",
      "2304/2304 [==============================] - 1s 297us/step - loss: 31256.0759 - val_loss: 28347.6139\n",
      "Epoch 60/1000\n",
      "2304/2304 [==============================] - 1s 308us/step - loss: 30887.1305 - val_loss: 29508.9473\n",
      "Epoch 61/1000\n",
      "2304/2304 [==============================] - 1s 309us/step - loss: 31034.4905 - val_loss: 28272.0954\n",
      "Epoch 62/1000\n",
      "2304/2304 [==============================] - 1s 289us/step - loss: 30804.1947 - val_loss: 28099.3891\n",
      "Epoch 63/1000\n",
      "2304/2304 [==============================] - 1s 289us/step - loss: 30715.4979 - val_loss: 28048.1679\n",
      "Epoch 64/1000\n",
      "2304/2304 [==============================] - 1s 309us/step - loss: 30798.3482 - val_loss: 27955.8191\n",
      "Epoch 65/1000\n",
      "2304/2304 [==============================] - 1s 284us/step - loss: 30704.6847 - val_loss: 27940.6464\n",
      "Epoch 66/1000\n",
      "2304/2304 [==============================] - 1s 309us/step - loss: 30808.3946 - val_loss: 28138.7246\n",
      "Epoch 67/1000\n",
      "2304/2304 [==============================] - 1s 280us/step - loss: 30849.4750 - val_loss: 27886.4417\n",
      "Epoch 68/1000\n",
      "2304/2304 [==============================] - 1s 316us/step - loss: 30589.7064 - val_loss: 28781.4515\n",
      "Epoch 69/1000\n",
      "2304/2304 [==============================] - 1s 321us/step - loss: 30647.6765 - val_loss: 28002.4028\n",
      "Epoch 70/1000\n",
      "2304/2304 [==============================] - 1s 307us/step - loss: 30680.1767 - val_loss: 27714.7210\n",
      "Epoch 71/1000\n",
      "2304/2304 [==============================] - 1s 299us/step - loss: 30736.4190 - val_loss: 28194.2515\n",
      "Epoch 72/1000\n",
      "2304/2304 [==============================] - 1s 326us/step - loss: 30684.7435 - val_loss: 27775.0184\n",
      "Epoch 73/1000\n",
      "2304/2304 [==============================] - 1s 341us/step - loss: 30342.8235 - val_loss: 27644.4381\n",
      "Epoch 74/1000\n",
      "2304/2304 [==============================] - 1s 297us/step - loss: 30440.8940 - val_loss: 27783.0751\n",
      "Epoch 75/1000\n",
      "2304/2304 [==============================] - 1s 298us/step - loss: 30088.1792 - val_loss: 27865.6164\n",
      "Epoch 76/1000\n",
      "2304/2304 [==============================] - 1s 287us/step - loss: 30431.3855 - val_loss: 28165.7769\n",
      "Epoch 77/1000\n",
      "2304/2304 [==============================] - 1s 290us/step - loss: 30042.7151 - val_loss: 27741.8484\n",
      "Epoch 78/1000\n",
      "2304/2304 [==============================] - 1s 299us/step - loss: 29995.7637 - val_loss: 27455.3913\n",
      "Epoch 79/1000\n",
      "2304/2304 [==============================] - 1s 255us/step - loss: 30103.3123 - val_loss: 27395.5136\n",
      "Epoch 80/1000\n",
      "2304/2304 [==============================] - 1s 243us/step - loss: 30170.5830 - val_loss: 27310.0103\n",
      "Epoch 81/1000\n",
      "2304/2304 [==============================] - 1s 283us/step - loss: 29793.8575 - val_loss: 28012.3408\n",
      "Epoch 82/1000\n",
      "2304/2304 [==============================] - 1s 295us/step - loss: 30324.6659 - val_loss: 27280.2126\n",
      "Epoch 83/1000\n",
      "2304/2304 [==============================] - 1s 302us/step - loss: 30044.1813 - val_loss: 27571.2918\n",
      "Epoch 84/1000\n",
      "2304/2304 [==============================] - 1s 261us/step - loss: 30047.0933 - val_loss: 26987.3099\n",
      "Epoch 85/1000\n",
      "2304/2304 [==============================] - 1s 231us/step - loss: 29852.9813 - val_loss: 27866.5595\n",
      "Epoch 86/1000\n",
      "2304/2304 [==============================] - 1s 236us/step - loss: 29833.6491 - val_loss: 27246.5312\n",
      "Epoch 87/1000\n",
      "2304/2304 [==============================] - 1s 236us/step - loss: 29878.0583 - val_loss: 27027.1467\n",
      "Epoch 88/1000\n",
      "2304/2304 [==============================] - 1s 247us/step - loss: 30054.2846 - val_loss: 27148.3252\n",
      "Epoch 89/1000\n",
      "2304/2304 [==============================] - 1s 248us/step - loss: 29886.7268 - val_loss: 27075.1215\n",
      "Epoch 90/1000\n",
      "2304/2304 [==============================] - 1s 261us/step - loss: 29744.9926 - val_loss: 26640.3245\n",
      "Epoch 91/1000\n",
      "2304/2304 [==============================] - 1s 244us/step - loss: 29543.0397 - val_loss: 26725.7906\n",
      "Epoch 92/1000\n",
      "2304/2304 [==============================] - 1s 303us/step - loss: 29696.6098 - val_loss: 27443.9266\n",
      "Epoch 93/1000\n",
      "2304/2304 [==============================] - 1s 310us/step - loss: 29651.1379 - val_loss: 26482.8411\n",
      "Epoch 94/1000\n",
      "2304/2304 [==============================] - 1s 317us/step - loss: 29281.3874 - val_loss: 26854.1230\n",
      "Epoch 95/1000\n",
      "2304/2304 [==============================] - 1s 278us/step - loss: 29505.6560 - val_loss: 26723.9112\n",
      "Epoch 96/1000\n",
      "2304/2304 [==============================] - 1s 291us/step - loss: 29379.1110 - val_loss: 26563.9606\n",
      "Epoch 97/1000\n",
      "2304/2304 [==============================] - 1s 252us/step - loss: 29256.5144 - val_loss: 27223.1555\n",
      "Epoch 98/1000\n",
      "2304/2304 [==============================] - 1s 334us/step - loss: 29314.8990 - val_loss: 26425.4241\n",
      "Epoch 99/1000\n",
      "2304/2304 [==============================] - 1s 321us/step - loss: 29231.0892 - val_loss: 26776.4689\n",
      "Epoch 100/1000\n",
      "2304/2304 [==============================] - 1s 273us/step - loss: 29056.2613 - val_loss: 26244.6343\n",
      "Epoch 101/1000\n",
      "2304/2304 [==============================] - 1s 300us/step - loss: 28746.1458 - val_loss: 26434.9306\n",
      "Epoch 102/1000\n",
      "2304/2304 [==============================] - 1s 305us/step - loss: 28865.3247 - val_loss: 26262.3733\n",
      "Epoch 103/1000\n",
      "2304/2304 [==============================] - 1s 277us/step - loss: 29101.8339 - val_loss: 26513.5930\n",
      "Epoch 104/1000\n",
      "2304/2304 [==============================] - 1s 245us/step - loss: 29101.2051 - val_loss: 25844.9742\n",
      "Epoch 105/1000\n",
      "2304/2304 [==============================] - 1s 262us/step - loss: 29050.3904 - val_loss: 25599.8770\n",
      "Epoch 106/1000\n",
      "2304/2304 [==============================] - 1s 322us/step - loss: 28844.7749 - val_loss: 26405.9459\n",
      "Epoch 107/1000\n",
      "2304/2304 [==============================] - 1s 334us/step - loss: 28550.1333 - val_loss: 26057.6616\n",
      "Epoch 108/1000\n",
      "2304/2304 [==============================] - 1s 281us/step - loss: 28462.0378 - val_loss: 25877.4473\n",
      "Epoch 109/1000\n",
      "2304/2304 [==============================] - 1s 309us/step - loss: 28734.4770 - val_loss: 25525.8683\n",
      "Epoch 110/1000\n",
      "2304/2304 [==============================] - 1s 311us/step - loss: 28809.2505 - val_loss: 25424.8050\n",
      "Epoch 111/1000\n",
      "2304/2304 [==============================] - 1s 308us/step - loss: 28497.7765 - val_loss: 25608.8335\n",
      "Epoch 112/1000\n",
      "2304/2304 [==============================] - 1s 260us/step - loss: 28404.0674 - val_loss: 25448.7847\n",
      "Epoch 113/1000\n",
      "2304/2304 [==============================] - 1s 263us/step - loss: 28602.3605 - val_loss: 27353.6342\n",
      "Epoch 114/1000\n",
      "2304/2304 [==============================] - 1s 252us/step - loss: 28038.4532 - val_loss: 25106.5988\n",
      "Epoch 115/1000\n",
      "2304/2304 [==============================] - 1s 242us/step - loss: 28204.1156 - val_loss: 25071.1677\n",
      "Epoch 116/1000\n",
      "2304/2304 [==============================] - 1s 248us/step - loss: 28112.9854 - val_loss: 25016.9271\n",
      "Epoch 117/1000\n",
      "2304/2304 [==============================] - 1s 298us/step - loss: 28254.4548 - val_loss: 24688.0922\n",
      "Epoch 118/1000\n",
      "2304/2304 [==============================] - 1s 282us/step - loss: 28052.5323 - val_loss: 24840.9854\n",
      "Epoch 119/1000\n",
      "2304/2304 [==============================] - 1s 218us/step - loss: 27959.5502 - val_loss: 24490.4330\n",
      "Epoch 120/1000\n",
      "2304/2304 [==============================] - 1s 229us/step - loss: 27590.5688 - val_loss: 24383.2620\n",
      "Epoch 121/1000\n",
      "2304/2304 [==============================] - 1s 303us/step - loss: 27908.5586 - val_loss: 24464.7424\n",
      "Epoch 122/1000\n",
      "2304/2304 [==============================] - 1s 265us/step - loss: 27987.0605 - val_loss: 24983.1090\n",
      "Epoch 123/1000\n",
      "2304/2304 [==============================] - 1s 315us/step - loss: 27753.6197 - val_loss: 25951.3489\n",
      "Epoch 124/1000\n",
      "2304/2304 [==============================] - 1s 258us/step - loss: 27561.9923 - val_loss: 23897.2297\n",
      "Epoch 125/1000\n",
      "2304/2304 [==============================] - 1s 263us/step - loss: 27447.7769 - val_loss: 24359.4837\n",
      "Epoch 126/1000\n",
      "2304/2304 [==============================] - 1s 319us/step - loss: 27181.8774 - val_loss: 23820.3173\n",
      "Epoch 127/1000\n",
      "2304/2304 [==============================] - 1s 329us/step - loss: 27246.8742 - val_loss: 23704.5092\n",
      "Epoch 128/1000\n",
      "2304/2304 [==============================] - 1s 296us/step - loss: 27091.6967 - val_loss: 23724.6449\n",
      "Epoch 129/1000\n",
      "2304/2304 [==============================] - 1s 254us/step - loss: 27160.1617 - val_loss: 23617.5038\n",
      "Epoch 130/1000\n",
      "2304/2304 [==============================] - 1s 233us/step - loss: 27044.9938 - val_loss: 24191.8003\n",
      "Epoch 131/1000\n",
      "2304/2304 [==============================] - 1s 234us/step - loss: 26789.7442 - val_loss: 23159.9871\n",
      "Epoch 132/1000\n",
      "2304/2304 [==============================] - 1s 274us/step - loss: 26873.1541 - val_loss: 27616.2691\n",
      "Epoch 133/1000\n",
      "2304/2304 [==============================] - 1s 266us/step - loss: 27070.9611 - val_loss: 23733.0335\n",
      "Epoch 134/1000\n",
      "2304/2304 [==============================] - 1s 333us/step - loss: 26757.0341 - val_loss: 23183.3805\n",
      "Epoch 135/1000\n",
      "2304/2304 [==============================] - 1s 317us/step - loss: 26676.2998 - val_loss: 22809.7488\n",
      "Epoch 136/1000\n",
      "2304/2304 [==============================] - 1s 320us/step - loss: 26832.1591 - val_loss: 23138.9999\n",
      "Epoch 137/1000\n",
      "2304/2304 [==============================] - 1s 273us/step - loss: 26252.2914 - val_loss: 22606.6924\n",
      "Epoch 138/1000\n",
      "2304/2304 [==============================] - 1s 300us/step - loss: 26894.2765 - val_loss: 23187.7622\n",
      "Epoch 139/1000\n",
      "2304/2304 [==============================] - 1s 255us/step - loss: 26231.5857 - val_loss: 22202.3165\n",
      "Epoch 140/1000\n",
      "2304/2304 [==============================] - 1s 266us/step - loss: 26137.5703 - val_loss: 22486.9044\n",
      "Epoch 141/1000\n",
      "2304/2304 [==============================] - 1s 324us/step - loss: 26231.8023 - val_loss: 22069.2678\n",
      "Epoch 142/1000\n",
      "2304/2304 [==============================] - 1s 321us/step - loss: 25927.4147 - val_loss: 21903.0942\n",
      "Epoch 143/1000\n",
      "2304/2304 [==============================] - 1s 306us/step - loss: 25726.6159 - val_loss: 21641.5089\n",
      "Epoch 144/1000\n",
      "2304/2304 [==============================] - 1s 317us/step - loss: 25694.5666 - val_loss: 23443.5498\n",
      "Epoch 145/1000\n",
      "2304/2304 [==============================] - 1s 265us/step - loss: 25528.7049 - val_loss: 22204.2959\n",
      "Epoch 146/1000\n",
      "2304/2304 [==============================] - 1s 235us/step - loss: 25878.4118 - val_loss: 22154.1091\n",
      "Epoch 147/1000\n",
      "2304/2304 [==============================] - 1s 266us/step - loss: 25599.6371 - val_loss: 21399.8729\n",
      "Epoch 148/1000\n",
      "2304/2304 [==============================] - 1s 318us/step - loss: 25275.6501 - val_loss: 21727.3690\n",
      "Epoch 149/1000\n",
      "2304/2304 [==============================] - 1s 241us/step - loss: 25279.0175 - val_loss: 22166.8561\n",
      "Epoch 150/1000\n",
      "2304/2304 [==============================] - 1s 250us/step - loss: 25205.7632 - val_loss: 21294.7585\n",
      "Epoch 151/1000\n",
      "2304/2304 [==============================] - 1s 331us/step - loss: 24859.9228 - val_loss: 22075.0598\n",
      "Epoch 152/1000\n",
      "2304/2304 [==============================] - 1s 295us/step - loss: 25093.8003 - val_loss: 20804.4427\n",
      "Epoch 153/1000\n",
      "2304/2304 [==============================] - 1s 247us/step - loss: 24596.3104 - val_loss: 20996.1663\n",
      "Epoch 154/1000\n",
      "2304/2304 [==============================] - 1s 249us/step - loss: 24641.2533 - val_loss: 20863.3916\n",
      "Epoch 155/1000\n",
      "2304/2304 [==============================] - 1s 289us/step - loss: 24889.0327 - val_loss: 21347.3896\n",
      "Epoch 156/1000\n",
      "2304/2304 [==============================] - 1s 289us/step - loss: 24867.0169 - val_loss: 22177.3831\n",
      "Epoch 157/1000\n",
      "2304/2304 [==============================] - 1s 292us/step - loss: 24749.7499 - val_loss: 21499.4589\n",
      "Epoch 158/1000\n",
      "2304/2304 [==============================] - 1s 329us/step - loss: 24782.7736 - val_loss: 22031.5829\n",
      "Epoch 159/1000\n",
      "2304/2304 [==============================] - 1s 301us/step - loss: 24501.1148 - val_loss: 20915.0504\n",
      "Epoch 160/1000\n",
      "2304/2304 [==============================] - 1s 307us/step - loss: 24739.7175 - val_loss: 20747.8368\n",
      "Epoch 161/1000\n",
      "2304/2304 [==============================] - 1s 319us/step - loss: 24645.3108 - val_loss: 21839.0587\n",
      "Epoch 162/1000\n",
      "2304/2304 [==============================] - 1s 318us/step - loss: 24446.5373 - val_loss: 20545.7103\n",
      "Epoch 163/1000\n",
      "2304/2304 [==============================] - 1s 326us/step - loss: 23998.7323 - val_loss: 19739.9767\n",
      "Epoch 164/1000\n",
      "2304/2304 [==============================] - 1s 330us/step - loss: 24104.2234 - val_loss: 20180.1322\n",
      "Epoch 165/1000\n",
      "2304/2304 [==============================] - 1s 305us/step - loss: 24264.8762 - val_loss: 20121.0390\n",
      "Epoch 166/1000\n",
      "2304/2304 [==============================] - 1s 305us/step - loss: 24758.0168 - val_loss: 19892.5074\n",
      "Epoch 167/1000\n",
      "2304/2304 [==============================] - 1s 322us/step - loss: 24145.5239 - val_loss: 25740.1654\n",
      "Epoch 168/1000\n",
      "2304/2304 [==============================] - 1s 253us/step - loss: 23809.7559 - val_loss: 20280.8955\n",
      "Epoch 169/1000\n",
      "2304/2304 [==============================] - 1s 267us/step - loss: 23775.6970 - val_loss: 20037.2912\n",
      "Epoch 170/1000\n",
      "2304/2304 [==============================] - 1s 303us/step - loss: 24031.8252 - val_loss: 21042.0565\n",
      "Epoch 171/1000\n",
      "2304/2304 [==============================] - 1s 325us/step - loss: 23822.5653 - val_loss: 19709.5257\n",
      "Epoch 172/1000\n",
      "2304/2304 [==============================] - 1s 316us/step - loss: 23633.1024 - val_loss: 21434.3345\n",
      "Epoch 173/1000\n",
      "2304/2304 [==============================] - 1s 227us/step - loss: 23524.6595 - val_loss: 19507.8303\n",
      "Epoch 174/1000\n",
      "2304/2304 [==============================] - 1s 241us/step - loss: 23708.0468 - val_loss: 22205.3141\n",
      "Epoch 175/1000\n",
      "2304/2304 [==============================] - 1s 271us/step - loss: 23932.4380 - val_loss: 19545.4745\n",
      "Epoch 176/1000\n",
      "2304/2304 [==============================] - 1s 292us/step - loss: 23196.6162 - val_loss: 19487.0450\n",
      "Epoch 177/1000\n",
      "2304/2304 [==============================] - 1s 252us/step - loss: 23446.3291 - val_loss: 18904.7570\n",
      "Epoch 178/1000\n",
      "2304/2304 [==============================] - 1s 233us/step - loss: 22928.9160 - val_loss: 19505.5781\n",
      "Epoch 179/1000\n",
      "2304/2304 [==============================] - 1s 279us/step - loss: 23622.4920 - val_loss: 18913.8879\n",
      "Epoch 180/1000\n",
      "2304/2304 [==============================] - 1s 307us/step - loss: 23565.2178 - val_loss: 20603.5325\n",
      "Epoch 181/1000\n",
      "2304/2304 [==============================] - 1s 294us/step - loss: 23218.8127 - val_loss: 19461.1357\n",
      "Epoch 182/1000\n",
      "2304/2304 [==============================] - 1s 313us/step - loss: 22788.5201 - val_loss: 19025.6312\n",
      "Epoch 183/1000\n",
      "2304/2304 [==============================] - 1s 296us/step - loss: 22574.2682 - val_loss: 18771.6541\n",
      "Epoch 184/1000\n",
      "2304/2304 [==============================] - 1s 301us/step - loss: 23825.6494 - val_loss: 19052.8107\n",
      "Epoch 185/1000\n",
      "2304/2304 [==============================] - 1s 258us/step - loss: 23103.7782 - val_loss: 18637.0292\n",
      "Epoch 186/1000\n",
      "2304/2304 [==============================] - 1s 295us/step - loss: 22956.9411 - val_loss: 18284.4380\n",
      "Epoch 187/1000\n",
      "2304/2304 [==============================] - 1s 318us/step - loss: 23295.6306 - val_loss: 18423.4671\n",
      "Epoch 188/1000\n",
      "2304/2304 [==============================] - 1s 283us/step - loss: 22959.5090 - val_loss: 18096.5715\n",
      "Epoch 189/1000\n",
      "2304/2304 [==============================] - 1s 302us/step - loss: 23165.4671 - val_loss: 18712.7886\n",
      "Epoch 190/1000\n",
      "2304/2304 [==============================] - 1s 294us/step - loss: 22993.4137 - val_loss: 21187.9364\n",
      "Epoch 191/1000\n",
      "2304/2304 [==============================] - 1s 286us/step - loss: 23045.5787 - val_loss: 18337.6754\n",
      "Epoch 192/1000\n",
      "2304/2304 [==============================] - 1s 322us/step - loss: 22653.8436 - val_loss: 18192.1547\n",
      "Epoch 193/1000\n",
      "2304/2304 [==============================] - 1s 294us/step - loss: 22815.2328 - val_loss: 20083.7538\n",
      "Epoch 194/1000\n",
      "2304/2304 [==============================] - 1s 334us/step - loss: 22608.0292 - val_loss: 20437.8893\n",
      "Epoch 195/1000\n",
      "2304/2304 [==============================] - 1s 320us/step - loss: 22559.4021 - val_loss: 18423.6167\n",
      "Epoch 196/1000\n",
      "2304/2304 [==============================] - 1s 321us/step - loss: 22560.1189 - val_loss: 18499.2182\n",
      "Epoch 197/1000\n",
      "2304/2304 [==============================] - 1s 306us/step - loss: 22351.4313 - val_loss: 18957.8034\n",
      "Epoch 198/1000\n",
      "2304/2304 [==============================] - 1s 250us/step - loss: 22659.0727 - val_loss: 22007.8815\n",
      "Epoch 199/1000\n",
      "2304/2304 [==============================] - 1s 264us/step - loss: 23298.8742 - val_loss: 20359.3317\n",
      "Epoch 200/1000\n",
      "2304/2304 [==============================] - 1s 247us/step - loss: 22099.8415 - val_loss: 18678.0943\n",
      "Epoch 201/1000\n",
      "2304/2304 [==============================] - 1s 239us/step - loss: 22351.0306 - val_loss: 19974.0519\n",
      "Epoch 202/1000\n",
      "2304/2304 [==============================] - 1s 234us/step - loss: 22559.6527 - val_loss: 17922.2026\n",
      "Epoch 203/1000\n",
      "2304/2304 [==============================] - 1s 252us/step - loss: 22361.9925 - val_loss: 21172.7884\n",
      "Epoch 204/1000\n",
      "2304/2304 [==============================] - 1s 299us/step - loss: 22441.3015 - val_loss: 17959.8514\n",
      "Epoch 205/1000\n",
      "2304/2304 [==============================] - 1s 262us/step - loss: 22056.4175 - val_loss: 19780.0659\n",
      "Epoch 206/1000\n",
      "2304/2304 [==============================] - 1s 263us/step - loss: 21745.4051 - val_loss: 18056.6594\n",
      "Epoch 207/1000\n",
      "2304/2304 [==============================] - 1s 327us/step - loss: 22243.8319 - val_loss: 18299.5056\n",
      "Epoch 208/1000\n",
      "2304/2304 [==============================] - 1s 282us/step - loss: 21992.8281 - val_loss: 18606.2277\n",
      "Epoch 209/1000\n",
      "2304/2304 [==============================] - 1s 286us/step - loss: 22177.0300 - val_loss: 20580.8157\n",
      "Epoch 210/1000\n",
      "2304/2304 [==============================] - 1s 286us/step - loss: 21870.1274 - val_loss: 17525.2455\n",
      "Epoch 211/1000\n",
      "2304/2304 [==============================] - 1s 262us/step - loss: 21573.0529 - val_loss: 19990.9594\n",
      "Epoch 212/1000\n",
      "2304/2304 [==============================] - 1s 325us/step - loss: 22516.5679 - val_loss: 22879.5714\n",
      "Epoch 213/1000\n",
      "2304/2304 [==============================] - 1s 227us/step - loss: 22100.7601 - val_loss: 17682.7765\n",
      "Epoch 214/1000\n",
      "2304/2304 [==============================] - 1s 256us/step - loss: 21758.8345 - val_loss: 18311.8781\n",
      "Epoch 215/1000\n",
      "2304/2304 [==============================] - 1s 289us/step - loss: 21676.7226 - val_loss: 17906.3438\n",
      "Epoch 216/1000\n",
      "2304/2304 [==============================] - 1s 304us/step - loss: 21567.2125 - val_loss: 17124.2677\n",
      "Epoch 217/1000\n",
      "2304/2304 [==============================] - 1s 325us/step - loss: 21714.8453 - val_loss: 19742.5807\n",
      "Epoch 218/1000\n",
      "2304/2304 [==============================] - 1s 256us/step - loss: 21751.5520 - val_loss: 18142.7294\n",
      "Epoch 219/1000\n",
      "2304/2304 [==============================] - 1s 252us/step - loss: 21455.3547 - val_loss: 19643.8214\n",
      "Epoch 220/1000\n",
      "2304/2304 [==============================] - 1s 256us/step - loss: 21609.3783 - val_loss: 17636.7450\n",
      "Epoch 221/1000\n",
      "2304/2304 [==============================] - 1s 307us/step - loss: 21777.9514 - val_loss: 18019.2534\n",
      "Epoch 222/1000\n",
      "2304/2304 [==============================] - 1s 317us/step - loss: 21738.8600 - val_loss: 17533.2229\n",
      "Epoch 223/1000\n",
      "2304/2304 [==============================] - 1s 274us/step - loss: 21661.8155 - val_loss: 17697.8317\n",
      "Epoch 224/1000\n",
      "2304/2304 [==============================] - 1s 249us/step - loss: 21329.0679 - val_loss: 17542.5039\n",
      "Epoch 225/1000\n",
      "2304/2304 [==============================] - 1s 263us/step - loss: 21660.7614 - val_loss: 17713.2470\n",
      "Epoch 226/1000\n",
      "2304/2304 [==============================] - 1s 300us/step - loss: 21472.7048 - val_loss: 18035.1684\n",
      "Epoch 227/1000\n",
      "2304/2304 [==============================] - 1s 263us/step - loss: 21553.2195 - val_loss: 18026.1552\n",
      "Epoch 228/1000\n",
      "2304/2304 [==============================] - 1s 232us/step - loss: 21161.3659 - val_loss: 20225.7631\n",
      "Epoch 229/1000\n",
      "2304/2304 [==============================] - 0s 169us/step - loss: 20997.3802 - val_loss: 17267.7994\n",
      "Epoch 230/1000\n",
      "2304/2304 [==============================] - 0s 155us/step - loss: 21879.1071 - val_loss: 17669.1867\n",
      "Epoch 231/1000\n",
      "2304/2304 [==============================] - 0s 196us/step - loss: 21235.0057 - val_loss: 16969.2727\n",
      "Epoch 232/1000\n",
      "2304/2304 [==============================] - 0s 214us/step - loss: 21178.0786 - val_loss: 17277.0912\n",
      "Epoch 233/1000\n",
      "2304/2304 [==============================] - 0s 208us/step - loss: 21231.6200 - val_loss: 17817.9829\n",
      "Epoch 234/1000\n",
      "2304/2304 [==============================] - 0s 212us/step - loss: 20680.1372 - val_loss: 18647.5089\n",
      "Epoch 235/1000\n",
      "2304/2304 [==============================] - 1s 250us/step - loss: 21357.9716 - val_loss: 17511.6428\n",
      "Epoch 236/1000\n",
      "2304/2304 [==============================] - 1s 226us/step - loss: 21337.3179 - val_loss: 25204.3843\n",
      "Epoch 237/1000\n",
      "2304/2304 [==============================] - 1s 222us/step - loss: 20942.0612 - val_loss: 17584.5581\n",
      "Epoch 238/1000\n",
      "2304/2304 [==============================] - 1s 219us/step - loss: 21148.7049 - val_loss: 17329.3806\n",
      "Epoch 239/1000\n",
      "2304/2304 [==============================] - 0s 201us/step - loss: 20981.6327 - val_loss: 18779.4103\n",
      "Epoch 240/1000\n",
      "2304/2304 [==============================] - 1s 218us/step - loss: 20819.9295 - val_loss: 17593.3755\n",
      "Epoch 241/1000\n",
      "2304/2304 [==============================] - 0s 202us/step - loss: 20673.1579 - val_loss: 18434.4467\n",
      "Epoch 242/1000\n",
      "2304/2304 [==============================] - 0s 187us/step - loss: 21229.9777 - val_loss: 17401.3100\n",
      "Epoch 243/1000\n",
      "2304/2304 [==============================] - 1s 221us/step - loss: 21035.1169 - val_loss: 19382.8562\n",
      "Epoch 244/1000\n",
      "2304/2304 [==============================] - 0s 208us/step - loss: 21116.5203 - val_loss: 17516.2690\n",
      "Epoch 245/1000\n",
      "2304/2304 [==============================] - 0s 156us/step - loss: 21566.1359 - val_loss: 19010.4701\n",
      "Epoch 246/1000\n",
      "2304/2304 [==============================] - 0s 182us/step - loss: 20504.4122 - val_loss: 19203.0506\n",
      "Epoch 247/1000\n",
      "2304/2304 [==============================] - 0s 195us/step - loss: 20907.0361 - val_loss: 16863.5147\n",
      "Epoch 248/1000\n",
      "2304/2304 [==============================] - 0s 183us/step - loss: 21297.2217 - val_loss: 16947.9680\n",
      "Epoch 249/1000\n",
      "2304/2304 [==============================] - 0s 195us/step - loss: 21205.1270 - val_loss: 17452.1372\n",
      "Epoch 250/1000\n",
      "2304/2304 [==============================] - 0s 192us/step - loss: 21079.6044 - val_loss: 16742.7520\n",
      "Epoch 251/1000\n",
      "2304/2304 [==============================] - 0s 194us/step - loss: 20697.4166 - val_loss: 16895.2733\n",
      "Epoch 252/1000\n",
      "2304/2304 [==============================] - 0s 188us/step - loss: 20613.7748 - val_loss: 17091.6867\n",
      "Epoch 253/1000\n",
      "2304/2304 [==============================] - 0s 194us/step - loss: 20305.7996 - val_loss: 16412.5784\n",
      "Epoch 254/1000\n",
      "2304/2304 [==============================] - 0s 212us/step - loss: 20827.3728 - val_loss: 16557.7535\n",
      "Epoch 255/1000\n",
      "2304/2304 [==============================] - 0s 177us/step - loss: 20598.8847 - val_loss: 18015.9641\n",
      "Epoch 256/1000\n",
      "2304/2304 [==============================] - 0s 169us/step - loss: 20803.1143 - val_loss: 18373.7819\n",
      "Epoch 257/1000\n",
      "2304/2304 [==============================] - 0s 189us/step - loss: 20627.0808 - val_loss: 17212.6795\n",
      "Epoch 258/1000\n",
      "2304/2304 [==============================] - 0s 188us/step - loss: 20978.6492 - val_loss: 16687.9252\n",
      "Epoch 259/1000\n",
      "2304/2304 [==============================] - 0s 194us/step - loss: 20656.4441 - val_loss: 19720.3247\n",
      "Epoch 260/1000\n",
      "2304/2304 [==============================] - 1s 238us/step - loss: 20497.2819 - val_loss: 17665.5034\n",
      "Epoch 261/1000\n",
      "2304/2304 [==============================] - 0s 207us/step - loss: 20496.9567 - val_loss: 19804.8422\n",
      "Epoch 262/1000\n",
      "2304/2304 [==============================] - 0s 204us/step - loss: 20678.4893 - val_loss: 17927.5507\n",
      "Epoch 263/1000\n",
      "2304/2304 [==============================] - 0s 205us/step - loss: 21029.6890 - val_loss: 19276.7184\n",
      "Epoch 264/1000\n",
      "2304/2304 [==============================] - 0s 203us/step - loss: 20158.5343 - val_loss: 16994.9395\n",
      "Epoch 265/1000\n",
      "2304/2304 [==============================] - 0s 203us/step - loss: 20028.6322 - val_loss: 16946.7359\n",
      "Epoch 266/1000\n",
      "2304/2304 [==============================] - 0s 179us/step - loss: 21355.6743 - val_loss: 16918.5789\n",
      "Epoch 267/1000\n",
      "2304/2304 [==============================] - 0s 158us/step - loss: 20768.2962 - val_loss: 16634.2811\n",
      "Epoch 268/1000\n",
      "2304/2304 [==============================] - 0s 166us/step - loss: 20604.5101 - val_loss: 16689.1931\n",
      "Epoch 269/1000\n",
      "2304/2304 [==============================] - 0s 153us/step - loss: 20772.4301 - val_loss: 16732.1983\n",
      "Epoch 270/1000\n",
      "2304/2304 [==============================] - 0s 175us/step - loss: 20699.6529 - val_loss: 16297.4794\n",
      "Epoch 271/1000\n",
      "2304/2304 [==============================] - 0s 178us/step - loss: 21085.4050 - val_loss: 17862.9947\n",
      "Epoch 272/1000\n",
      "2304/2304 [==============================] - 0s 182us/step - loss: 20357.8873 - val_loss: 16533.8325\n",
      "Epoch 273/1000\n",
      "2304/2304 [==============================] - 0s 199us/step - loss: 21358.7567 - val_loss: 16633.8474\n",
      "Epoch 274/1000\n",
      "2304/2304 [==============================] - 0s 165us/step - loss: 20352.0598 - val_loss: 18201.2850\n",
      "Epoch 275/1000\n",
      "2304/2304 [==============================] - 0s 178us/step - loss: 20663.8623 - val_loss: 17929.8486\n",
      "Epoch 276/1000\n",
      "2304/2304 [==============================] - 0s 179us/step - loss: 19895.4757 - val_loss: 16853.7624\n",
      "Epoch 277/1000\n",
      "2304/2304 [==============================] - 0s 163us/step - loss: 20383.7297 - val_loss: 16311.2625\n",
      "Epoch 278/1000\n",
      "2304/2304 [==============================] - 0s 210us/step - loss: 20327.3415 - val_loss: 16724.9611\n",
      "Epoch 279/1000\n",
      "2304/2304 [==============================] - 0s 158us/step - loss: 20156.0959 - val_loss: 16629.0600\n",
      "Epoch 280/1000\n",
      "2304/2304 [==============================] - 0s 165us/step - loss: 20002.6718 - val_loss: 22096.3132\n",
      "Epoch 281/1000\n",
      "2304/2304 [==============================] - 0s 189us/step - loss: 20416.1880 - val_loss: 15936.5998\n",
      "Epoch 282/1000\n",
      "2304/2304 [==============================] - 0s 162us/step - loss: 20366.0029 - val_loss: 16765.1146\n",
      "Epoch 283/1000\n",
      "2304/2304 [==============================] - 0s 204us/step - loss: 20421.1256 - val_loss: 17365.2083\n",
      "Epoch 284/1000\n",
      "2304/2304 [==============================] - 0s 193us/step - loss: 20107.2872 - val_loss: 16528.1010\n",
      "Epoch 285/1000\n",
      "2304/2304 [==============================] - 1s 233us/step - loss: 20038.0222 - val_loss: 18047.7319\n",
      "Epoch 286/1000\n",
      "2304/2304 [==============================] - 0s 207us/step - loss: 20253.4689 - val_loss: 16016.4676\n",
      "Epoch 287/1000\n",
      "2304/2304 [==============================] - 1s 228us/step - loss: 20171.3477 - val_loss: 15992.5876\n",
      "Epoch 288/1000\n",
      "2304/2304 [==============================] - 0s 213us/step - loss: 19697.0379 - val_loss: 16655.0589\n",
      "Epoch 289/1000\n",
      "2304/2304 [==============================] - 0s 206us/step - loss: 19913.6451 - val_loss: 17706.2218\n",
      "Epoch 290/1000\n",
      "2304/2304 [==============================] - 0s 215us/step - loss: 19961.8242 - val_loss: 17446.1676\n",
      "Epoch 291/1000\n",
      "2304/2304 [==============================] - 0s 216us/step - loss: 20098.7043 - val_loss: 15662.2244\n",
      "Epoch 292/1000\n",
      "2304/2304 [==============================] - 1s 233us/step - loss: 20069.2137 - val_loss: 17185.7049\n",
      "Epoch 293/1000\n",
      "2304/2304 [==============================] - 0s 201us/step - loss: 19985.4707 - val_loss: 17558.7681\n",
      "Epoch 294/1000\n",
      "2304/2304 [==============================] - 0s 188us/step - loss: 20180.2450 - val_loss: 17656.7542\n",
      "Epoch 295/1000\n",
      "2304/2304 [==============================] - 0s 191us/step - loss: 19882.8481 - val_loss: 18026.7505\n",
      "Epoch 296/1000\n",
      "2304/2304 [==============================] - 0s 178us/step - loss: 19859.8945 - val_loss: 18113.6885\n",
      "Epoch 297/1000\n",
      "2304/2304 [==============================] - 0s 155us/step - loss: 19597.0581 - val_loss: 15951.7516\n",
      "Epoch 298/1000\n",
      "2304/2304 [==============================] - 0s 156us/step - loss: 19598.6594 - val_loss: 15719.8099\n",
      "Epoch 299/1000\n",
      "2304/2304 [==============================] - 0s 209us/step - loss: 19830.0233 - val_loss: 16424.4954\n",
      "Epoch 300/1000\n",
      "2304/2304 [==============================] - 0s 214us/step - loss: 20055.3755 - val_loss: 20551.3620\n",
      "Epoch 301/1000\n",
      "2304/2304 [==============================] - 1s 237us/step - loss: 19850.9565 - val_loss: 16019.4625\n",
      "Epoch 302/1000\n",
      "2304/2304 [==============================] - 0s 200us/step - loss: 19972.9314 - val_loss: 16181.6447\n",
      "Epoch 303/1000\n",
      "2304/2304 [==============================] - 0s 196us/step - loss: 19419.8595 - val_loss: 15552.7936\n",
      "Epoch 304/1000\n",
      "2304/2304 [==============================] - 0s 210us/step - loss: 19539.9303 - val_loss: 15805.7057\n",
      "Epoch 305/1000\n",
      "2304/2304 [==============================] - 0s 201us/step - loss: 19698.2437 - val_loss: 21233.2846\n",
      "Epoch 306/1000\n",
      "2304/2304 [==============================] - 0s 216us/step - loss: 20054.3103 - val_loss: 15569.1453\n",
      "Epoch 307/1000\n",
      "2304/2304 [==============================] - 0s 157us/step - loss: 20113.0694 - val_loss: 15391.3149\n",
      "Epoch 308/1000\n",
      "2304/2304 [==============================] - 0s 161us/step - loss: 19218.0109 - val_loss: 16703.9333\n",
      "Epoch 309/1000\n",
      "2304/2304 [==============================] - 0s 194us/step - loss: 19378.8664 - val_loss: 16145.4901\n",
      "Epoch 310/1000\n",
      "2304/2304 [==============================] - 0s 209us/step - loss: 19605.6502 - val_loss: 18036.8542\n",
      "Epoch 311/1000\n",
      "2304/2304 [==============================] - 1s 228us/step - loss: 19304.6242 - val_loss: 16020.4908\n",
      "Epoch 312/1000\n",
      "2304/2304 [==============================] - 0s 192us/step - loss: 19733.2518 - val_loss: 15838.1647\n",
      "Epoch 313/1000\n",
      "2304/2304 [==============================] - 0s 215us/step - loss: 19894.1271 - val_loss: 21623.8247\n",
      "Epoch 314/1000\n",
      "2304/2304 [==============================] - 0s 211us/step - loss: 19313.3879 - val_loss: 15826.0538\n",
      "Epoch 315/1000\n",
      "2304/2304 [==============================] - 0s 209us/step - loss: 20215.3929 - val_loss: 15432.8428\n",
      "Epoch 316/1000\n",
      "2304/2304 [==============================] - 0s 205us/step - loss: 19776.9174 - val_loss: 17258.9779\n",
      "Epoch 317/1000\n",
      "2304/2304 [==============================] - 1s 228us/step - loss: 19447.4485 - val_loss: 15376.7857\n",
      "Epoch 318/1000\n",
      "2304/2304 [==============================] - 0s 214us/step - loss: 19467.2618 - val_loss: 21326.5444\n",
      "Epoch 319/1000\n",
      "2304/2304 [==============================] - 1s 223us/step - loss: 19727.2119 - val_loss: 16627.9851\n",
      "Epoch 320/1000\n",
      "2304/2304 [==============================] - 1s 234us/step - loss: 19382.7458 - val_loss: 15779.4601\n",
      "Epoch 321/1000\n",
      "2304/2304 [==============================] - 0s 208us/step - loss: 19256.2187 - val_loss: 15231.2185\n",
      "Epoch 322/1000\n",
      "2304/2304 [==============================] - 1s 217us/step - loss: 19071.4380 - val_loss: 15473.1322\n",
      "Epoch 323/1000\n",
      "2304/2304 [==============================] - 0s 182us/step - loss: 18777.4293 - val_loss: 15169.6853\n",
      "Epoch 324/1000\n",
      "2304/2304 [==============================] - 0s 214us/step - loss: 18843.1509 - val_loss: 15448.9617\n",
      "Epoch 325/1000\n",
      "2304/2304 [==============================] - 0s 185us/step - loss: 19019.3462 - val_loss: 15406.0035\n",
      "Epoch 326/1000\n",
      "2304/2304 [==============================] - 0s 214us/step - loss: 19087.0144 - val_loss: 15275.7321\n",
      "Epoch 327/1000\n",
      "2304/2304 [==============================] - 1s 220us/step - loss: 19304.2754 - val_loss: 15063.9202\n",
      "Epoch 328/1000\n",
      "2304/2304 [==============================] - 0s 194us/step - loss: 19427.5658 - val_loss: 15750.2823\n",
      "Epoch 329/1000\n",
      "2304/2304 [==============================] - 0s 191us/step - loss: 19104.2138 - val_loss: 15179.6253\n",
      "Epoch 330/1000\n",
      "2304/2304 [==============================] - 0s 182us/step - loss: 19560.6787 - val_loss: 15031.5000\n",
      "Epoch 331/1000\n",
      "2304/2304 [==============================] - 0s 153us/step - loss: 19217.1736 - val_loss: 17138.9938\n",
      "Epoch 332/1000\n",
      "2304/2304 [==============================] - 0s 168us/step - loss: 19633.6949 - val_loss: 17383.4283\n",
      "Epoch 333/1000\n",
      "2304/2304 [==============================] - 0s 155us/step - loss: 19101.8393 - val_loss: 17143.5466\n",
      "Epoch 334/1000\n",
      "2304/2304 [==============================] - 0s 174us/step - loss: 19141.7926 - val_loss: 14950.8007\n",
      "Epoch 335/1000\n",
      "2304/2304 [==============================] - 0s 178us/step - loss: 18894.9056 - val_loss: 15397.0469\n",
      "Epoch 336/1000\n",
      "2304/2304 [==============================] - 0s 194us/step - loss: 19797.9212 - val_loss: 14891.6816\n",
      "Epoch 337/1000\n",
      "2304/2304 [==============================] - 0s 194us/step - loss: 19126.2471 - val_loss: 14417.2211\n",
      "Epoch 338/1000\n",
      "2304/2304 [==============================] - 0s 187us/step - loss: 18719.4568 - val_loss: 17648.2148\n",
      "Epoch 339/1000\n",
      "2304/2304 [==============================] - 0s 210us/step - loss: 19574.6821 - val_loss: 15511.7548\n",
      "Epoch 340/1000\n",
      "2304/2304 [==============================] - 0s 179us/step - loss: 19346.5488 - val_loss: 14579.7716\n",
      "Epoch 341/1000\n",
      "2304/2304 [==============================] - 0s 204us/step - loss: 19012.4305 - val_loss: 14800.2543\n",
      "Epoch 342/1000\n",
      "2304/2304 [==============================] - 0s 206us/step - loss: 18913.3318 - val_loss: 15262.1326\n",
      "Epoch 343/1000\n",
      "2304/2304 [==============================] - 1s 234us/step - loss: 19120.3380 - val_loss: 18169.2064\n",
      "Epoch 344/1000\n",
      "2304/2304 [==============================] - 0s 198us/step - loss: 19505.6018 - val_loss: 14882.6963\n",
      "Epoch 345/1000\n",
      "2304/2304 [==============================] - 0s 190us/step - loss: 19046.4969 - val_loss: 15718.2154\n",
      "Epoch 346/1000\n",
      "2304/2304 [==============================] - 1s 225us/step - loss: 18735.2936 - val_loss: 15377.4719\n",
      "Epoch 347/1000\n",
      "2304/2304 [==============================] - 0s 184us/step - loss: 18634.6204 - val_loss: 14929.0376\n",
      "Epoch 348/1000\n",
      "2304/2304 [==============================] - 0s 191us/step - loss: 19597.8797 - val_loss: 15112.3727\n",
      "Epoch 349/1000\n",
      "2304/2304 [==============================] - 0s 204us/step - loss: 18876.8138 - val_loss: 15388.4135\n",
      "Epoch 350/1000\n",
      "2304/2304 [==============================] - 0s 214us/step - loss: 18817.6496 - val_loss: 15468.4895\n",
      "Epoch 351/1000\n",
      "2304/2304 [==============================] - 0s 194us/step - loss: 19157.8014 - val_loss: 17307.6890\n",
      "Epoch 352/1000\n",
      "2304/2304 [==============================] - 0s 194us/step - loss: 18844.3125 - val_loss: 17616.2412\n",
      "Epoch 353/1000\n",
      "2304/2304 [==============================] - 1s 223us/step - loss: 19034.0637 - val_loss: 14666.3903\n",
      "Epoch 354/1000\n",
      "2304/2304 [==============================] - 0s 217us/step - loss: 19193.8054 - val_loss: 17320.4331\n",
      "Epoch 355/1000\n",
      "2304/2304 [==============================] - 1s 241us/step - loss: 19284.3807 - val_loss: 17481.6944\n",
      "Epoch 356/1000\n",
      "2304/2304 [==============================] - 1s 224us/step - loss: 19130.1773 - val_loss: 14782.5616\n",
      "Epoch 357/1000\n",
      "2304/2304 [==============================] - 1s 245us/step - loss: 18147.2251 - val_loss: 15306.2728\n",
      "Epoch 358/1000\n",
      "2304/2304 [==============================] - 0s 210us/step - loss: 18490.1018 - val_loss: 16162.0027\n",
      "Epoch 359/1000\n",
      "2304/2304 [==============================] - 1s 244us/step - loss: 18591.0676 - val_loss: 15209.2175\n",
      "Epoch 360/1000\n",
      "2304/2304 [==============================] - 1s 230us/step - loss: 18842.1015 - val_loss: 17660.0668\n",
      "Epoch 361/1000\n",
      "2304/2304 [==============================] - 1s 236us/step - loss: 18298.5297 - val_loss: 14595.1418\n",
      "Epoch 362/1000\n",
      "2304/2304 [==============================] - 0s 190us/step - loss: 18617.4753 - val_loss: 17199.7666\n",
      "Epoch 363/1000\n",
      "2304/2304 [==============================] - 0s 198us/step - loss: 18790.2485 - val_loss: 14904.9109\n",
      "Epoch 364/1000\n",
      "2304/2304 [==============================] - 0s 194us/step - loss: 18978.0750 - val_loss: 14864.7450\n",
      "Epoch 365/1000\n",
      "2304/2304 [==============================] - 0s 186us/step - loss: 18555.2245 - val_loss: 19020.9972\n",
      "Epoch 366/1000\n",
      "2304/2304 [==============================] - 0s 166us/step - loss: 18470.5474 - val_loss: 18759.5096\n",
      "Epoch 367/1000\n",
      "2304/2304 [==============================] - 0s 152us/step - loss: 19013.8387 - val_loss: 14362.9641\n",
      "Epoch 368/1000\n",
      "2304/2304 [==============================] - 0s 165us/step - loss: 18560.5738 - val_loss: 14298.3401\n",
      "Epoch 369/1000\n",
      "2304/2304 [==============================] - 0s 204us/step - loss: 18088.8933 - val_loss: 14763.0484\n",
      "Epoch 370/1000\n",
      "2304/2304 [==============================] - 0s 199us/step - loss: 18984.2312 - val_loss: 14330.1759\n",
      "Epoch 371/1000\n",
      "2304/2304 [==============================] - 0s 190us/step - loss: 18630.5968 - val_loss: 17003.4683\n",
      "Epoch 372/1000\n",
      "2304/2304 [==============================] - 0s 164us/step - loss: 18833.2708 - val_loss: 14910.8756\n",
      "Epoch 373/1000\n",
      "2304/2304 [==============================] - 0s 156us/step - loss: 18590.4829 - val_loss: 14486.1008\n",
      "Epoch 374/1000\n",
      "2304/2304 [==============================] - 0s 172us/step - loss: 19213.6731 - val_loss: 14410.4953\n",
      "Epoch 375/1000\n",
      "2304/2304 [==============================] - 0s 195us/step - loss: 18714.4162 - val_loss: 14952.9407\n",
      "Epoch 376/1000\n",
      "2304/2304 [==============================] - 1s 250us/step - loss: 18673.8623 - val_loss: 15718.1266\n",
      "Epoch 377/1000\n",
      "2304/2304 [==============================] - 1s 231us/step - loss: 18422.1978 - val_loss: 14979.0351\n",
      "Epoch 378/1000\n",
      "2304/2304 [==============================] - 1s 230us/step - loss: 18142.3148 - val_loss: 15348.6446\n",
      "Epoch 379/1000\n",
      "2304/2304 [==============================] - 0s 189us/step - loss: 18898.3536 - val_loss: 15283.5788\n",
      "Epoch 380/1000\n",
      "2304/2304 [==============================] - 0s 215us/step - loss: 18142.7385 - val_loss: 16035.1628\n",
      "Epoch 381/1000\n",
      "2304/2304 [==============================] - 0s 201us/step - loss: 18083.8400 - val_loss: 14695.2663\n",
      "Epoch 382/1000\n",
      "2304/2304 [==============================] - 1s 237us/step - loss: 18194.6073 - val_loss: 14761.2252\n",
      "Epoch 383/1000\n",
      "2304/2304 [==============================] - 0s 215us/step - loss: 17993.0316 - val_loss: 14604.0477\n",
      "Epoch 384/1000\n",
      "2304/2304 [==============================] - 1s 230us/step - loss: 18160.1157 - val_loss: 15107.6087\n",
      "Epoch 385/1000\n",
      "2304/2304 [==============================] - 0s 208us/step - loss: 18177.9474 - val_loss: 16811.6828\n",
      "Epoch 386/1000\n",
      "2304/2304 [==============================] - 1s 219us/step - loss: 18421.7973 - val_loss: 14526.8860\n",
      "Epoch 387/1000\n",
      "2304/2304 [==============================] - 0s 213us/step - loss: 18866.9204 - val_loss: 15139.2751\n",
      "Epoch 388/1000\n",
      "2304/2304 [==============================] - 0s 180us/step - loss: 19431.8118 - val_loss: 19777.2861\n",
      "Epoch 389/1000\n",
      "2304/2304 [==============================] - 0s 213us/step - loss: 18486.4246 - val_loss: 14880.8153\n",
      "Epoch 390/1000\n",
      "2304/2304 [==============================] - 0s 166us/step - loss: 18834.7748 - val_loss: 14539.0616\n",
      "Epoch 391/1000\n",
      "2304/2304 [==============================] - 0s 191us/step - loss: 18112.3404 - val_loss: 14447.9604\n",
      "Epoch 392/1000\n",
      "2304/2304 [==============================] - 0s 196us/step - loss: 18123.6201 - val_loss: 15238.6208\n",
      "Epoch 393/1000\n",
      "2304/2304 [==============================] - 0s 215us/step - loss: 18005.1497 - val_loss: 14599.7878\n",
      "Epoch 394/1000\n",
      "2304/2304 [==============================] - 0s 213us/step - loss: 18209.5132 - val_loss: 14894.9281\n",
      "Epoch 395/1000\n",
      "2304/2304 [==============================] - 0s 186us/step - loss: 18783.0851 - val_loss: 14864.9141\n",
      "Epoch 396/1000\n",
      "2304/2304 [==============================] - 1s 220us/step - loss: 18259.1900 - val_loss: 14166.5452\n",
      "Epoch 397/1000\n",
      "2304/2304 [==============================] - 0s 184us/step - loss: 17919.9789 - val_loss: 15161.9159\n",
      "Epoch 398/1000\n",
      "2304/2304 [==============================] - 0s 181us/step - loss: 18085.4525 - val_loss: 15678.4499\n",
      "Epoch 399/1000\n",
      "2304/2304 [==============================] - 0s 191us/step - loss: 17690.6998 - val_loss: 14256.7773\n",
      "Epoch 400/1000\n",
      "2304/2304 [==============================] - 0s 160us/step - loss: 17635.9954 - val_loss: 18720.1307\n",
      "Epoch 401/1000\n",
      "2304/2304 [==============================] - 0s 168us/step - loss: 17792.3375 - val_loss: 14681.2616\n",
      "Epoch 402/1000\n",
      "2304/2304 [==============================] - 0s 189us/step - loss: 18241.8167 - val_loss: 15251.2717\n",
      "Epoch 403/1000\n",
      "2304/2304 [==============================] - 0s 190us/step - loss: 18104.7483 - val_loss: 14546.1720\n",
      "Epoch 404/1000\n",
      "2304/2304 [==============================] - 0s 193us/step - loss: 19194.0794 - val_loss: 13930.9687\n",
      "Epoch 405/1000\n",
      "2304/2304 [==============================] - 0s 177us/step - loss: 18094.3390 - val_loss: 13663.1932\n",
      "Epoch 406/1000\n",
      "2304/2304 [==============================] - 0s 198us/step - loss: 17958.5851 - val_loss: 15067.7854\n",
      "Epoch 407/1000\n",
      "2304/2304 [==============================] - 0s 186us/step - loss: 17822.3275 - val_loss: 14811.1929\n",
      "Epoch 408/1000\n",
      "2304/2304 [==============================] - 0s 205us/step - loss: 17817.2098 - val_loss: 14207.1298\n",
      "Epoch 409/1000\n",
      "2304/2304 [==============================] - 0s 198us/step - loss: 17867.1089 - val_loss: 14563.2494\n",
      "Epoch 410/1000\n",
      "2304/2304 [==============================] - 0s 191us/step - loss: 17609.3387 - val_loss: 15659.5599\n",
      "Epoch 411/1000\n",
      "2304/2304 [==============================] - 0s 213us/step - loss: 18132.4921 - val_loss: 14562.0808\n",
      "Epoch 412/1000\n",
      "2304/2304 [==============================] - 0s 165us/step - loss: 17635.0106 - val_loss: 15144.5789\n",
      "Epoch 413/1000\n",
      "2304/2304 [==============================] - 0s 215us/step - loss: 18156.6439 - val_loss: 16131.4904\n",
      "Epoch 414/1000\n",
      "2304/2304 [==============================] - 0s 197us/step - loss: 18653.5280 - val_loss: 20236.1129\n",
      "Epoch 415/1000\n",
      "2304/2304 [==============================] - 0s 214us/step - loss: 17933.6877 - val_loss: 14544.3319\n",
      "Epoch 416/1000\n",
      "2304/2304 [==============================] - 0s 195us/step - loss: 17437.3738 - val_loss: 13938.3645\n",
      "Epoch 417/1000\n",
      "2304/2304 [==============================] - 0s 161us/step - loss: 18064.0344 - val_loss: 14752.3999\n",
      "Epoch 418/1000\n",
      "2304/2304 [==============================] - 0s 212us/step - loss: 18164.5799 - val_loss: 14176.7333\n",
      "Epoch 419/1000\n",
      "2304/2304 [==============================] - 0s 173us/step - loss: 17751.0832 - val_loss: 14782.1766\n",
      "Epoch 420/1000\n",
      "2304/2304 [==============================] - 0s 212us/step - loss: 18056.0899 - val_loss: 14383.0286\n",
      "Epoch 421/1000\n",
      "2304/2304 [==============================] - 0s 191us/step - loss: 18390.7836 - val_loss: 13918.0640\n",
      "Epoch 422/1000\n",
      "2304/2304 [==============================] - 0s 184us/step - loss: 17445.4808 - val_loss: 15001.5611\n",
      "Epoch 423/1000\n",
      "2304/2304 [==============================] - 0s 210us/step - loss: 17591.0891 - val_loss: 17972.1628\n",
      "Epoch 424/1000\n",
      "2304/2304 [==============================] - 0s 213us/step - loss: 18236.8050 - val_loss: 14581.3150\n",
      "Epoch 425/1000\n",
      "2304/2304 [==============================] - 1s 236us/step - loss: 17344.4306 - val_loss: 15619.9008\n",
      "Epoch 426/1000\n",
      "2304/2304 [==============================] - 0s 194us/step - loss: 17881.1189 - val_loss: 15241.8802\n",
      "Epoch 427/1000\n",
      "2304/2304 [==============================] - 1s 244us/step - loss: 18370.4654 - val_loss: 18666.6838\n",
      "Epoch 428/1000\n",
      "2304/2304 [==============================] - 1s 217us/step - loss: 18296.6935 - val_loss: 15756.4844\n",
      "Epoch 429/1000\n",
      "2304/2304 [==============================] - 1s 253us/step - loss: 18004.8936 - val_loss: 16697.5932\n",
      "Epoch 430/1000\n",
      "2304/2304 [==============================] - 0s 206us/step - loss: 18178.1255 - val_loss: 14346.6202\n",
      "Epoch 431/1000\n",
      "2304/2304 [==============================] - 0s 216us/step - loss: 17471.4738 - val_loss: 15409.3710\n",
      "Epoch 432/1000\n",
      "2304/2304 [==============================] - 0s 214us/step - loss: 17716.9743 - val_loss: 18953.6699\n",
      "Epoch 433/1000\n",
      "2304/2304 [==============================] - 0s 187us/step - loss: 17345.1759 - val_loss: 13956.5311\n",
      "Epoch 434/1000\n",
      "2304/2304 [==============================] - 0s 184us/step - loss: 17680.6646 - val_loss: 13937.4271\n",
      "Epoch 435/1000\n",
      "2304/2304 [==============================] - 0s 145us/step - loss: 18085.8132 - val_loss: 17019.6446\n",
      "Epoch 436/1000\n",
      "2304/2304 [==============================] - 0s 170us/step - loss: 18037.2839 - val_loss: 14090.6176\n",
      "Epoch 437/1000\n",
      "2304/2304 [==============================] - 0s 167us/step - loss: 18131.3250 - val_loss: 15653.2680\n",
      "Epoch 438/1000\n",
      "2304/2304 [==============================] - 0s 168us/step - loss: 18533.1117 - val_loss: 14161.6732\n",
      "Epoch 439/1000\n",
      "2304/2304 [==============================] - 0s 201us/step - loss: 18316.1555 - val_loss: 15031.0142\n",
      "Epoch 440/1000\n",
      "2304/2304 [==============================] - 0s 186us/step - loss: 17709.8951 - val_loss: 15586.6958\n",
      "Epoch 441/1000\n",
      "2304/2304 [==============================] - 1s 218us/step - loss: 17577.6349 - val_loss: 15071.3500\n",
      "Epoch 442/1000\n",
      "2304/2304 [==============================] - 0s 184us/step - loss: 17548.4594 - val_loss: 14024.1006\n",
      "Epoch 443/1000\n",
      "2304/2304 [==============================] - 0s 212us/step - loss: 17425.4228 - val_loss: 13805.6418\n",
      "Epoch 444/1000\n",
      "2304/2304 [==============================] - 0s 197us/step - loss: 17583.2738 - val_loss: 14343.3232\n",
      "Epoch 445/1000\n",
      "2304/2304 [==============================] - 0s 174us/step - loss: 17544.6698 - val_loss: 15018.0226\n",
      "Epoch 446/1000\n",
      "2304/2304 [==============================] - 1s 229us/step - loss: 17988.9432 - val_loss: 15291.9328\n",
      "Epoch 447/1000\n",
      "2304/2304 [==============================] - 0s 206us/step - loss: 17834.2986 - val_loss: 14942.1053\n",
      "Epoch 448/1000\n",
      "2304/2304 [==============================] - 1s 254us/step - loss: 17767.6650 - val_loss: 14309.0718\n",
      "Epoch 449/1000\n",
      "2304/2304 [==============================] - 0s 195us/step - loss: 18254.9460 - val_loss: 14041.4823\n",
      "Epoch 450/1000\n",
      "2304/2304 [==============================] - 1s 240us/step - loss: 17526.4044 - val_loss: 15159.9130\n",
      "Epoch 451/1000\n",
      "2304/2304 [==============================] - 0s 194us/step - loss: 17691.2356 - val_loss: 17196.7177\n",
      "Epoch 452/1000\n",
      "2304/2304 [==============================] - 1s 224us/step - loss: 17584.5048 - val_loss: 13824.5303\n",
      "Epoch 453/1000\n",
      "2304/2304 [==============================] - 0s 188us/step - loss: 18635.0768 - val_loss: 13833.3146\n",
      "Epoch 454/1000\n",
      "2304/2304 [==============================] - 0s 184us/step - loss: 18102.8852 - val_loss: 16987.0251\n",
      "Epoch 455/1000\n",
      "2304/2304 [==============================] - 0s 208us/step - loss: 17810.3009 - val_loss: 14111.7356\n",
      "Epoch 456/1000\n",
      "2304/2304 [==============================] - 0s 185us/step - loss: 17435.8452 - val_loss: 14719.4318\n",
      "Epoch 457/1000\n",
      "2304/2304 [==============================] - 1s 237us/step - loss: 17638.5186 - val_loss: 14466.3419\n",
      "Epoch 458/1000\n",
      "2304/2304 [==============================] - 0s 186us/step - loss: 17350.2892 - val_loss: 14286.3167\n",
      "Epoch 459/1000\n",
      "2304/2304 [==============================] - 0s 212us/step - loss: 17986.4063 - val_loss: 14030.3092\n",
      "Epoch 460/1000\n",
      "2304/2304 [==============================] - 0s 200us/step - loss: 17411.3769 - val_loss: 15086.7227\n",
      "Epoch 461/1000\n",
      "2304/2304 [==============================] - 1s 228us/step - loss: 17637.8072 - val_loss: 14763.3945\n",
      "Epoch 462/1000\n",
      "2304/2304 [==============================] - 0s 203us/step - loss: 17078.5205 - val_loss: 14857.0905\n",
      "Epoch 463/1000\n",
      "2304/2304 [==============================] - 0s 192us/step - loss: 17486.2746 - val_loss: 14221.9409\n",
      "Epoch 464/1000\n",
      "2304/2304 [==============================] - 0s 207us/step - loss: 17693.2992 - val_loss: 19086.9787\n",
      "Epoch 465/1000\n",
      "2304/2304 [==============================] - 0s 207us/step - loss: 17330.4516 - val_loss: 16933.6912\n",
      "Epoch 466/1000\n",
      "2304/2304 [==============================] - 1s 233us/step - loss: 17539.6286 - val_loss: 13948.7500\n",
      "Epoch 467/1000\n",
      "2304/2304 [==============================] - 0s 193us/step - loss: 17798.2716 - val_loss: 15387.0951\n",
      "Epoch 468/1000\n",
      "2304/2304 [==============================] - 0s 186us/step - loss: 17286.0338 - val_loss: 13805.1669\n",
      "Epoch 469/1000\n",
      "2304/2304 [==============================] - 0s 174us/step - loss: 18044.1464 - val_loss: 14089.9121\n",
      "Epoch 470/1000\n",
      "2304/2304 [==============================] - 0s 151us/step - loss: 17550.2268 - val_loss: 14451.5191\n",
      "Epoch 471/1000\n",
      "2304/2304 [==============================] - 0s 167us/step - loss: 17434.6112 - val_loss: 13879.8280\n",
      "Epoch 472/1000\n",
      "2304/2304 [==============================] - 0s 172us/step - loss: 17151.0645 - val_loss: 17152.1364\n",
      "Epoch 473/1000\n",
      "2304/2304 [==============================] - 0s 192us/step - loss: 17233.1623 - val_loss: 14373.1381\n",
      "Epoch 474/1000\n",
      "2304/2304 [==============================] - 0s 187us/step - loss: 17317.8637 - val_loss: 13950.1936\n",
      "Epoch 475/1000\n",
      "2304/2304 [==============================] - 0s 181us/step - loss: 17809.6433 - val_loss: 13643.1785\n",
      "Epoch 476/1000\n",
      "2304/2304 [==============================] - 0s 212us/step - loss: 17202.2175 - val_loss: 13681.6053\n",
      "Epoch 477/1000\n",
      "2304/2304 [==============================] - 0s 184us/step - loss: 17653.4310 - val_loss: 16367.7475\n",
      "Epoch 478/1000\n",
      "2304/2304 [==============================] - 0s 195us/step - loss: 17604.7257 - val_loss: 18169.3009\n",
      "Epoch 479/1000\n",
      "2304/2304 [==============================] - 0s 204us/step - loss: 17408.5373 - val_loss: 14683.8751\n",
      "Epoch 480/1000\n",
      "2304/2304 [==============================] - 0s 209us/step - loss: 17312.4812 - val_loss: 14794.9911\n",
      "Epoch 481/1000\n",
      "2304/2304 [==============================] - 0s 211us/step - loss: 17397.9574 - val_loss: 14134.3538\n",
      "Epoch 482/1000\n",
      "2304/2304 [==============================] - 0s 182us/step - loss: 17108.0568 - val_loss: 14444.4669\n",
      "Epoch 483/1000\n",
      "2304/2304 [==============================] - 0s 199us/step - loss: 17421.9430 - val_loss: 14362.7498\n",
      "Epoch 484/1000\n",
      "2304/2304 [==============================] - 0s 186us/step - loss: 17242.3430 - val_loss: 14349.3988\n",
      "Epoch 485/1000\n",
      "2304/2304 [==============================] - 0s 192us/step - loss: 17950.9676 - val_loss: 15838.1022\n",
      "Epoch 486/1000\n",
      "2304/2304 [==============================] - 0s 181us/step - loss: 17061.1899 - val_loss: 14143.3934\n",
      "Epoch 487/1000\n",
      "2304/2304 [==============================] - 0s 204us/step - loss: 18118.6220 - val_loss: 13696.2311\n",
      "Epoch 488/1000\n",
      "2304/2304 [==============================] - 0s 217us/step - loss: 16924.3521 - val_loss: 15466.7771\n",
      "Epoch 489/1000\n",
      "2304/2304 [==============================] - 0s 186us/step - loss: 16947.9790 - val_loss: 14163.9034\n",
      "Epoch 490/1000\n",
      "2304/2304 [==============================] - 0s 169us/step - loss: 17506.1105 - val_loss: 21157.0539\n",
      "Epoch 491/1000\n",
      "2304/2304 [==============================] - 0s 191us/step - loss: 17619.1805 - val_loss: 16658.5208\n",
      "Epoch 492/1000\n",
      "2304/2304 [==============================] - 0s 189us/step - loss: 17214.6719 - val_loss: 16983.9025\n",
      "Epoch 493/1000\n",
      "2304/2304 [==============================] - 0s 197us/step - loss: 16840.1872 - val_loss: 14273.4900\n",
      "Epoch 494/1000\n",
      "2304/2304 [==============================] - 0s 171us/step - loss: 17232.1410 - val_loss: 15815.8045\n",
      "Epoch 495/1000\n",
      "2304/2304 [==============================] - 1s 219us/step - loss: 17063.3449 - val_loss: 13824.9720\n",
      "Epoch 496/1000\n",
      "2304/2304 [==============================] - 0s 213us/step - loss: 16950.5687 - val_loss: 14295.5610\n",
      "Epoch 497/1000\n",
      "2304/2304 [==============================] - 0s 216us/step - loss: 17432.8902 - val_loss: 14195.9899\n",
      "Epoch 498/1000\n",
      "2304/2304 [==============================] - 0s 209us/step - loss: 17445.2511 - val_loss: 17265.7731\n",
      "Epoch 499/1000\n",
      "2304/2304 [==============================] - 0s 208us/step - loss: 16961.3527 - val_loss: 14674.3004\n",
      "Epoch 500/1000\n",
      "2304/2304 [==============================] - 0s 191us/step - loss: 17144.2767 - val_loss: 14173.4049\n",
      "Epoch 501/1000\n",
      "2304/2304 [==============================] - 0s 192us/step - loss: 16976.5612 - val_loss: 14043.8272\n",
      "Epoch 502/1000\n",
      "2304/2304 [==============================] - 1s 231us/step - loss: 16911.0966 - val_loss: 15623.2592\n",
      "Epoch 503/1000\n",
      "2304/2304 [==============================] - 0s 186us/step - loss: 16777.3428 - val_loss: 14794.0727\n",
      "Epoch 504/1000\n",
      "2304/2304 [==============================] - 0s 188us/step - loss: 17123.8859 - val_loss: 13899.2233\n",
      "Epoch 505/1000\n",
      "2304/2304 [==============================] - 0s 163us/step - loss: 17548.5512 - val_loss: 14392.6133\n",
      "Epoch 506/1000\n",
      "2304/2304 [==============================] - 0s 156us/step - loss: 17633.4414 - val_loss: 18892.5295\n",
      "Epoch 507/1000\n",
      "2304/2304 [==============================] - 0s 215us/step - loss: 17625.3353 - val_loss: 15150.0644\n",
      "Epoch 508/1000\n",
      "2304/2304 [==============================] - 0s 190us/step - loss: 16847.9232 - val_loss: 14338.2441\n",
      "Epoch 509/1000\n",
      "2304/2304 [==============================] - 1s 220us/step - loss: 16733.0168 - val_loss: 14141.7374\n",
      "Epoch 510/1000\n",
      "2304/2304 [==============================] - 0s 200us/step - loss: 16923.7001 - val_loss: 14743.8228\n",
      "Epoch 511/1000\n",
      "2304/2304 [==============================] - 1s 240us/step - loss: 17290.7347 - val_loss: 15737.1522\n",
      "Epoch 512/1000\n",
      "2304/2304 [==============================] - 0s 204us/step - loss: 17172.0514 - val_loss: 17647.0372\n",
      "Epoch 513/1000\n",
      "2304/2304 [==============================] - 1s 218us/step - loss: 16380.3861 - val_loss: 14447.7533\n",
      "Epoch 514/1000\n",
      "2304/2304 [==============================] - 0s 189us/step - loss: 16858.5221 - val_loss: 14291.9939\n",
      "Epoch 515/1000\n",
      "2304/2304 [==============================] - 0s 161us/step - loss: 17048.4059 - val_loss: 15412.5023\n",
      "Epoch 516/1000\n",
      "2304/2304 [==============================] - 0s 174us/step - loss: 17673.6034 - val_loss: 14833.8032\n",
      "Epoch 517/1000\n",
      "2304/2304 [==============================] - 0s 169us/step - loss: 17647.4735 - val_loss: 17166.9072\n",
      "Epoch 518/1000\n",
      "2304/2304 [==============================] - 0s 205us/step - loss: 17389.3983 - val_loss: 14512.7775\n",
      "Epoch 519/1000\n",
      "2304/2304 [==============================] - 0s 167us/step - loss: 17341.1414 - val_loss: 17246.1255\n",
      "Epoch 520/1000\n",
      "2304/2304 [==============================] - 0s 168us/step - loss: 17109.1732 - val_loss: 16566.4963\n",
      "Epoch 521/1000\n",
      "2304/2304 [==============================] - 0s 160us/step - loss: 16652.8098 - val_loss: 14065.9372\n",
      "Epoch 522/1000\n",
      "2304/2304 [==============================] - 0s 177us/step - loss: 16770.3193 - val_loss: 14835.8756\n",
      "Epoch 523/1000\n",
      "2304/2304 [==============================] - 0s 194us/step - loss: 17017.4603 - val_loss: 17272.6756\n",
      "Epoch 524/1000\n",
      "2304/2304 [==============================] - 0s 208us/step - loss: 16773.3493 - val_loss: 14492.4565\n",
      "Epoch 525/1000\n",
      "2304/2304 [==============================] - 0s 158us/step - loss: 16656.6626 - val_loss: 14382.4491\n",
      "Epoch 526/1000\n",
      "2304/2304 [==============================] - 0s 160us/step - loss: 17119.5806 - val_loss: 14469.4266\n",
      "Epoch 527/1000\n",
      "2304/2304 [==============================] - 0s 181us/step - loss: 16510.5726 - val_loss: 14436.2811\n",
      "Epoch 528/1000\n",
      "2304/2304 [==============================] - 0s 187us/step - loss: 17212.8729 - val_loss: 14233.1776\n",
      "Epoch 529/1000\n",
      "2304/2304 [==============================] - ETA: 0s - loss: 16832.486 - 0s 191us/step - loss: 17107.8964 - val_loss: 20408.9563\n",
      "Epoch 530/1000\n",
      "2304/2304 [==============================] - 0s 166us/step - loss: 17256.0299 - val_loss: 14655.3525\n",
      "Epoch 531/1000\n",
      "2304/2304 [==============================] - 0s 182us/step - loss: 16885.7371 - val_loss: 14413.6153\n",
      "Epoch 532/1000\n",
      "2304/2304 [==============================] - 0s 183us/step - loss: 17450.4201 - val_loss: 17652.2493\n",
      "Epoch 533/1000\n",
      "2304/2304 [==============================] - 0s 178us/step - loss: 16829.5802 - val_loss: 18942.8990\n",
      "Epoch 534/1000\n",
      "2304/2304 [==============================] - 0s 188us/step - loss: 16732.0117 - val_loss: 18350.9705\n",
      "Epoch 535/1000\n",
      "2304/2304 [==============================] - 0s 191us/step - loss: 17111.7821 - val_loss: 14748.4955\n",
      "Epoch 536/1000\n",
      "2304/2304 [==============================] - 0s 184us/step - loss: 16824.7655 - val_loss: 17367.9273\n",
      "Epoch 537/1000\n",
      "2304/2304 [==============================] - 0s 179us/step - loss: 17200.5611 - val_loss: 16078.3114\n",
      "Epoch 538/1000\n",
      "2304/2304 [==============================] - 0s 177us/step - loss: 17074.8697 - val_loss: 18277.4364\n",
      "Epoch 539/1000\n",
      "2304/2304 [==============================] - 0s 153us/step - loss: 16601.2790 - val_loss: 14852.7733\n",
      "Epoch 540/1000\n",
      "2304/2304 [==============================] - 0s 189us/step - loss: 16786.4261 - val_loss: 15381.2632\n",
      "Epoch 541/1000\n",
      "2304/2304 [==============================] - 0s 195us/step - loss: 16568.8286 - val_loss: 15601.1657\n",
      "Epoch 542/1000\n",
      "2304/2304 [==============================] - 0s 183us/step - loss: 16738.1293 - val_loss: 14778.8433\n",
      "Epoch 543/1000\n",
      "2304/2304 [==============================] - 0s 153us/step - loss: 16434.4178 - val_loss: 15024.3458\n",
      "Epoch 544/1000\n",
      "2304/2304 [==============================] - 0s 162us/step - loss: 16897.1505 - val_loss: 14294.6896\n",
      "Epoch 545/1000\n",
      "2304/2304 [==============================] - 0s 172us/step - loss: 16744.1907 - val_loss: 14506.5594\n",
      "Epoch 546/1000\n",
      "2304/2304 [==============================] - 0s 182us/step - loss: 16517.5628 - val_loss: 14370.0524\n",
      "Epoch 547/1000\n",
      "2304/2304 [==============================] - 0s 167us/step - loss: 16862.6958 - val_loss: 22211.1258\n",
      "Epoch 548/1000\n",
      "2304/2304 [==============================] - 0s 188us/step - loss: 17070.6574 - val_loss: 14618.9036\n",
      "Epoch 549/1000\n",
      "2304/2304 [==============================] - 0s 198us/step - loss: 16504.4241 - val_loss: 14628.4153\n",
      "Epoch 550/1000\n",
      "2304/2304 [==============================] - 0s 163us/step - loss: 16809.8601 - val_loss: 16447.5517\n",
      "Epoch 551/1000\n",
      "2304/2304 [==============================] - 0s 153us/step - loss: 16495.1639 - val_loss: 14687.0393\n",
      "Epoch 552/1000\n",
      "2304/2304 [==============================] - 0s 163us/step - loss: 16285.5014 - val_loss: 14138.7878\n",
      "Epoch 553/1000\n",
      "2304/2304 [==============================] - 0s 175us/step - loss: 16753.7172 - val_loss: 14539.7864\n",
      "Epoch 554/1000\n",
      "2304/2304 [==============================] - 0s 156us/step - loss: 16650.5657 - val_loss: 16424.9093\n",
      "Epoch 555/1000\n",
      "2304/2304 [==============================] - 0s 181us/step - loss: 16886.8158 - val_loss: 15472.9316\n",
      "Epoch 556/1000\n",
      "2304/2304 [==============================] - 0s 174us/step - loss: 16813.1082 - val_loss: 15359.7454\n",
      "Epoch 557/1000\n",
      "2304/2304 [==============================] - 0s 165us/step - loss: 16330.1410 - val_loss: 14645.6081\n",
      "Epoch 558/1000\n",
      "2304/2304 [==============================] - 0s 188us/step - loss: 16647.2237 - val_loss: 14699.9120\n",
      "Epoch 559/1000\n",
      "2304/2304 [==============================] - 0s 162us/step - loss: 16993.7356 - val_loss: 15157.0326\n",
      "Epoch 560/1000\n",
      "2304/2304 [==============================] - 0s 167us/step - loss: 16269.7229 - val_loss: 14550.8012\n",
      "Epoch 561/1000\n",
      "2304/2304 [==============================] - 0s 188us/step - loss: 16442.6503 - val_loss: 15242.3677\n",
      "Epoch 562/1000\n",
      "2304/2304 [==============================] - 0s 165us/step - loss: 16358.9158 - val_loss: 16408.2398\n",
      "Epoch 563/1000\n",
      "2304/2304 [==============================] - 0s 217us/step - loss: 16925.1536 - val_loss: 16144.2732\n",
      "Epoch 564/1000\n",
      "2304/2304 [==============================] - 0s 190us/step - loss: 17284.7159 - val_loss: 20048.0430\n",
      "Epoch 565/1000\n",
      "2304/2304 [==============================] - 1s 232us/step - loss: 17251.4449 - val_loss: 15486.9565\n",
      "Epoch 566/1000\n",
      "2304/2304 [==============================] - 1s 222us/step - loss: 16804.5295 - val_loss: 14647.2811\n",
      "Epoch 567/1000\n",
      "2304/2304 [==============================] - 1s 217us/step - loss: 16754.5402 - val_loss: 19237.2444\n",
      "Epoch 568/1000\n",
      "2304/2304 [==============================] - 0s 158us/step - loss: 16467.5124 - val_loss: 15691.5164\n",
      "Epoch 569/1000\n",
      "2304/2304 [==============================] - 0s 175us/step - loss: 16970.9215 - val_loss: 17327.8458\n",
      "Epoch 570/1000\n",
      "2304/2304 [==============================] - 1s 219us/step - loss: 16184.7426 - val_loss: 15217.6621\n",
      "Epoch 571/1000\n",
      "2304/2304 [==============================] - 0s 192us/step - loss: 17201.9462 - val_loss: 14681.4576\n",
      "Epoch 572/1000\n",
      "2304/2304 [==============================] - 0s 194us/step - loss: 16931.9812 - val_loss: 15972.9878\n",
      "Epoch 573/1000\n",
      "2304/2304 [==============================] - 0s 188us/step - loss: 17166.8520 - val_loss: 15172.4244\n",
      "Epoch 574/1000\n",
      "2304/2304 [==============================] - 0s 197us/step - loss: 16998.9853 - val_loss: 21011.2664\n",
      "Epoch 575/1000\n",
      "2304/2304 [==============================] - 0s 216us/step - loss: 17016.8627 - val_loss: 14770.8945\n",
      "Epoch 576/1000\n",
      "2304/2304 [==============================] - 1s 217us/step - loss: 16683.6465 - val_loss: 15127.6091\n",
      "Epoch 577/1000\n",
      "2304/2304 [==============================] - 1s 228us/step - loss: 16371.7950 - val_loss: 17321.0513\n",
      "Epoch 578/1000\n",
      "2304/2304 [==============================] - 0s 190us/step - loss: 17622.5744 - val_loss: 14896.8862\n",
      "Epoch 579/1000\n",
      "2304/2304 [==============================] - 0s 190us/step - loss: 16475.0848 - val_loss: 15604.2618\n",
      "Epoch 580/1000\n",
      "2304/2304 [==============================] - 0s 162us/step - loss: 16682.8052 - val_loss: 15098.4965\n",
      "Epoch 581/1000\n",
      "2304/2304 [==============================] - 0s 167us/step - loss: 17052.6825 - val_loss: 15214.5319\n",
      "Epoch 582/1000\n",
      "2304/2304 [==============================] - 0s 191us/step - loss: 16584.2289 - val_loss: 14421.7351\n",
      "Epoch 583/1000\n",
      "2304/2304 [==============================] - 0s 168us/step - loss: 16844.5556 - val_loss: 14642.4289\n",
      "Epoch 584/1000\n",
      "2304/2304 [==============================] - 0s 203us/step - loss: 16605.5554 - val_loss: 16078.5492\n",
      "Epoch 585/1000\n",
      "2304/2304 [==============================] - 0s 208us/step - loss: 16328.7630 - val_loss: 15847.3440\n",
      "Epoch 586/1000\n",
      "2304/2304 [==============================] - 1s 232us/step - loss: 16649.6390 - val_loss: 14558.9477\n",
      "Epoch 587/1000\n",
      "2304/2304 [==============================] - 0s 204us/step - loss: 16255.2002 - val_loss: 15075.7664\n",
      "Epoch 588/1000\n",
      "2304/2304 [==============================] - 1s 236us/step - loss: 16293.5971 - val_loss: 14692.7911\n",
      "Epoch 589/1000\n",
      "2304/2304 [==============================] - 0s 201us/step - loss: 16410.9915 - val_loss: 17439.0140\n",
      "Epoch 590/1000\n",
      "2304/2304 [==============================] - 0s 184us/step - loss: 16273.7043 - val_loss: 14951.5752\n",
      "Epoch 591/1000\n",
      "2304/2304 [==============================] - 1s 226us/step - loss: 16474.4618 - val_loss: 15009.6657\n",
      "Epoch 592/1000\n",
      "2304/2304 [==============================] - 1s 224us/step - loss: 16678.6295 - val_loss: 14603.4350\n",
      "Epoch 593/1000\n",
      "2304/2304 [==============================] - 1s 233us/step - loss: 16349.3562 - val_loss: 16450.9091\n",
      "Epoch 594/1000\n",
      "2304/2304 [==============================] - 0s 214us/step - loss: 16251.6263 - val_loss: 15379.4591\n",
      "Epoch 595/1000\n",
      "2304/2304 [==============================] - 1s 229us/step - loss: 15952.7027 - val_loss: 14398.4968\n",
      "Epoch 596/1000\n",
      "2304/2304 [==============================] - 0s 207us/step - loss: 16684.7073 - val_loss: 18667.4691\n",
      "Epoch 597/1000\n",
      "2304/2304 [==============================] - 1s 236us/step - loss: 16194.5132 - val_loss: 15947.2477\n",
      "Epoch 598/1000\n",
      "2304/2304 [==============================] - 0s 182us/step - loss: 16762.9995 - val_loss: 16047.1045\n",
      "Epoch 599/1000\n",
      "2304/2304 [==============================] - 0s 213us/step - loss: 16759.3350 - val_loss: 19615.8501\n",
      "Epoch 600/1000\n",
      "2304/2304 [==============================] - 0s 198us/step - loss: 16779.4033 - val_loss: 15635.6562\n",
      "Epoch 601/1000\n",
      "2304/2304 [==============================] - 0s 204us/step - loss: 16068.9117 - val_loss: 14839.8837\n",
      "Epoch 602/1000\n",
      "2304/2304 [==============================] - 0s 202us/step - loss: 16580.0770 - val_loss: 20236.2722\n",
      "Epoch 603/1000\n",
      "2304/2304 [==============================] - 0s 212us/step - loss: 17825.7435 - val_loss: 15947.8983\n",
      "Epoch 604/1000\n",
      "2304/2304 [==============================] - 0s 212us/step - loss: 16636.5912 - val_loss: 15431.7372\n",
      "Epoch 605/1000\n",
      "2304/2304 [==============================] - 0s 181us/step - loss: 16558.4262 - val_loss: 14733.0653\n",
      "Epoch 606/1000\n",
      "2304/2304 [==============================] - 0s 210us/step - loss: 16857.8711 - val_loss: 16049.8758\n",
      "Epoch 607/1000\n",
      "2304/2304 [==============================] - 0s 197us/step - loss: 16176.6660 - val_loss: 16382.9235\n",
      "Epoch 608/1000\n",
      "2304/2304 [==============================] - 0s 214us/step - loss: 16814.4212 - val_loss: 15287.7825\n",
      "Epoch 609/1000\n",
      "2304/2304 [==============================] - 0s 194us/step - loss: 15917.8620 - val_loss: 14814.1185\n",
      "Epoch 610/1000\n",
      "2304/2304 [==============================] - 0s 201us/step - loss: 16631.5449 - val_loss: 15193.6708\n",
      "Epoch 611/1000\n",
      "2304/2304 [==============================] - 1s 221us/step - loss: 16689.0758 - val_loss: 15157.2404\n",
      "Epoch 612/1000\n",
      "2304/2304 [==============================] - 0s 181us/step - loss: 17105.4514 - val_loss: 15289.7617\n",
      "Epoch 613/1000\n",
      "2304/2304 [==============================] - 0s 187us/step - loss: 16732.5733 - val_loss: 15188.8082\n",
      "Epoch 614/1000\n",
      "2304/2304 [==============================] - 0s 166us/step - loss: 16482.4875 - val_loss: 14929.0551\n",
      "Epoch 615/1000\n",
      "2304/2304 [==============================] - 0s 146us/step - loss: 17717.7374 - val_loss: 14843.8612\n",
      "Epoch 616/1000\n",
      "2304/2304 [==============================] - 0s 179us/step - loss: 16653.0561 - val_loss: 17560.8496\n",
      "Epoch 617/1000\n",
      "2304/2304 [==============================] - 0s 154us/step - loss: 16571.7404 - val_loss: 16518.7149\n",
      "Epoch 618/1000\n",
      "2304/2304 [==============================] - 0s 177us/step - loss: 16932.5716 - val_loss: 14910.2646\n",
      "Epoch 619/1000\n",
      "2304/2304 [==============================] - 0s 206us/step - loss: 16785.4350 - val_loss: 15628.8247\n",
      "Epoch 620/1000\n",
      "2304/2304 [==============================] - 0s 194us/step - loss: 16648.4839 - val_loss: 15319.4646\n",
      "Epoch 621/1000\n",
      "2304/2304 [==============================] - 1s 228us/step - loss: 16377.3623 - val_loss: 15035.5879\n",
      "Epoch 622/1000\n",
      "2304/2304 [==============================] - 0s 166us/step - loss: 16160.2855 - val_loss: 14719.2775\n",
      "Epoch 623/1000\n",
      "2304/2304 [==============================] - 0s 201us/step - loss: 16508.3157 - val_loss: 15165.0231\n",
      "Epoch 624/1000\n",
      "2304/2304 [==============================] - 1s 241us/step - loss: 16329.1945 - val_loss: 14750.8376\n",
      "Epoch 625/1000\n",
      "2304/2304 [==============================] - 0s 175us/step - loss: 16532.3880 - val_loss: 19824.8312\n",
      "Epoch 626/1000\n",
      "2304/2304 [==============================] - 0s 154us/step - loss: 16196.7769 - val_loss: 17051.2055\n",
      "Epoch 627/1000\n",
      "2304/2304 [==============================] - 0s 156us/step - loss: 16497.0991 - val_loss: 15499.6341\n",
      "Epoch 628/1000\n",
      "2304/2304 [==============================] - 0s 181us/step - loss: 16366.7026 - val_loss: 14652.8497\n",
      "Epoch 629/1000\n",
      "2304/2304 [==============================] - 0s 181us/step - loss: 17149.8921 - val_loss: 15504.3284\n",
      "Epoch 630/1000\n",
      "2304/2304 [==============================] - 0s 181us/step - loss: 16381.7322 - val_loss: 15344.0974\n",
      "Epoch 631/1000\n",
      "2304/2304 [==============================] - 0s 193us/step - loss: 15954.7157 - val_loss: 15514.7185\n",
      "Epoch 632/1000\n",
      "2304/2304 [==============================] - 0s 168us/step - loss: 16154.6259 - val_loss: 15149.8128\n",
      "Epoch 633/1000\n",
      "2304/2304 [==============================] - 0s 184us/step - loss: 16119.8522 - val_loss: 17409.4823\n",
      "Epoch 634/1000\n",
      "2304/2304 [==============================] - 0s 181us/step - loss: 16267.2105 - val_loss: 15001.5716\n",
      "Epoch 635/1000\n",
      "2304/2304 [==============================] - 0s 169us/step - loss: 15634.5691 - val_loss: 15654.8408\n",
      "Epoch 636/1000\n",
      "2304/2304 [==============================] - 0s 210us/step - loss: 16204.5983 - val_loss: 15497.9529\n",
      "Epoch 637/1000\n",
      "2304/2304 [==============================] - 0s 167us/step - loss: 15956.1646 - val_loss: 14726.4365\n",
      "Epoch 638/1000\n",
      "2304/2304 [==============================] - 1s 222us/step - loss: 15692.2452 - val_loss: 15571.6100\n",
      "Epoch 639/1000\n",
      "2304/2304 [==============================] - 0s 212us/step - loss: 15762.9378 - val_loss: 15176.7802\n",
      "Epoch 640/1000\n",
      "2304/2304 [==============================] - 1s 228us/step - loss: 16490.8685 - val_loss: 16446.4146\n",
      "Epoch 641/1000\n",
      "2304/2304 [==============================] - 0s 213us/step - loss: 16361.0326 - val_loss: 15478.5634\n",
      "Epoch 642/1000\n",
      "2304/2304 [==============================] - 0s 213us/step - loss: 16187.8661 - val_loss: 15124.0546\n",
      "Epoch 643/1000\n",
      "2304/2304 [==============================] - 0s 216us/step - loss: 15872.0582 - val_loss: 16070.6285\n",
      "Epoch 644/1000\n",
      "2304/2304 [==============================] - 1s 234us/step - loss: 16205.5067 - val_loss: 21267.3795\n",
      "Epoch 645/1000\n",
      "2304/2304 [==============================] - 1s 227us/step - loss: 16216.1856 - val_loss: 22789.1562\n",
      "Epoch 646/1000\n",
      "2304/2304 [==============================] - 1s 227us/step - loss: 16441.9670 - val_loss: 15154.2708\n",
      "Epoch 647/1000\n",
      "2304/2304 [==============================] - 1s 231us/step - loss: 15890.7162 - val_loss: 15580.2044\n",
      "Epoch 648/1000\n",
      "2304/2304 [==============================] - 0s 183us/step - loss: 16672.2616 - val_loss: 15079.8580\n",
      "Epoch 649/1000\n",
      "2304/2304 [==============================] - 0s 175us/step - loss: 15809.7910 - val_loss: 17558.7879\n",
      "Epoch 650/1000\n",
      "2304/2304 [==============================] - 0s 160us/step - loss: 16717.2909 - val_loss: 15334.8993\n",
      "Epoch 651/1000\n",
      "2304/2304 [==============================] - 0s 163us/step - loss: 16723.7221 - val_loss: 17872.9211\n",
      "Epoch 652/1000\n",
      "2304/2304 [==============================] - 0s 204us/step - loss: 16465.3446 - val_loss: 15142.7462\n",
      "Epoch 653/1000\n",
      "2304/2304 [==============================] - 0s 213us/step - loss: 16049.0883 - val_loss: 14990.8490\n",
      "Epoch 654/1000\n",
      "2304/2304 [==============================] - 1s 232us/step - loss: 16265.8294 - val_loss: 20468.1871\n",
      "Epoch 655/1000\n",
      "2304/2304 [==============================] - 0s 182us/step - loss: 15972.7062 - val_loss: 14849.3802\n",
      "Epoch 656/1000\n",
      "2304/2304 [==============================] - 1s 240us/step - loss: 16820.6931 - val_loss: 16038.1227\n",
      "Epoch 657/1000\n",
      "2304/2304 [==============================] - 0s 212us/step - loss: 16026.5599 - val_loss: 15826.9530\n",
      "Epoch 658/1000\n",
      "2304/2304 [==============================] - 0s 217us/step - loss: 16323.5041 - val_loss: 15138.0333\n",
      "Epoch 659/1000\n",
      "2304/2304 [==============================] - 0s 204us/step - loss: 15872.7743 - val_loss: 15205.3234\n",
      "Epoch 660/1000\n",
      "2304/2304 [==============================] - 1s 253us/step - loss: 16575.0575 - val_loss: 15016.4981\n",
      "Epoch 661/1000\n",
      "2304/2304 [==============================] - 0s 201us/step - loss: 15930.5196 - val_loss: 14802.9534\n",
      "Epoch 662/1000\n",
      "2304/2304 [==============================] - 0s 214us/step - loss: 16127.0641 - val_loss: 15067.5519\n",
      "Epoch 663/1000\n",
      "2304/2304 [==============================] - 0s 201us/step - loss: 16433.4832 - val_loss: 18844.4809\n",
      "Epoch 664/1000\n",
      "2304/2304 [==============================] - 0s 173us/step - loss: 16118.1720 - val_loss: 15553.8743\n",
      "Epoch 665/1000\n",
      "2304/2304 [==============================] - 1s 235us/step - loss: 16312.5253 - val_loss: 14551.7005\n",
      "Epoch 666/1000\n",
      "2304/2304 [==============================] - 0s 169us/step - loss: 16224.7265 - val_loss: 15007.3819\n",
      "Epoch 667/1000\n",
      "2304/2304 [==============================] - 0s 174us/step - loss: 16008.0637 - val_loss: 14803.8108\n",
      "Epoch 668/1000\n",
      "2304/2304 [==============================] - 0s 194us/step - loss: 16320.1771 - val_loss: 15928.2509\n",
      "Epoch 669/1000\n",
      "2304/2304 [==============================] - 0s 191us/step - loss: 15987.9259 - val_loss: 15353.4260\n",
      "Epoch 670/1000\n",
      "2304/2304 [==============================] - 0s 213us/step - loss: 16228.8002 - val_loss: 15127.7993\n",
      "Epoch 671/1000\n",
      "2304/2304 [==============================] - 0s 182us/step - loss: 16460.3939 - val_loss: 15020.2346\n",
      "Epoch 672/1000\n",
      "2304/2304 [==============================] - 0s 180us/step - loss: 16480.7836 - val_loss: 17069.0662\n",
      "Epoch 673/1000\n",
      "2304/2304 [==============================] - 0s 185us/step - loss: 15987.9948 - val_loss: 14667.7120\n",
      "Epoch 674/1000\n",
      "2304/2304 [==============================] - 0s 195us/step - loss: 15911.9206 - val_loss: 16384.8135\n",
      "Epoch 675/1000\n",
      "2304/2304 [==============================] - 1s 223us/step - loss: 15889.1764 - val_loss: 15356.8043\n",
      "Epoch 676/1000\n",
      "2304/2304 [==============================] - 0s 201us/step - loss: 16185.7914 - val_loss: 16003.0194\n",
      "Epoch 677/1000\n",
      "2304/2304 [==============================] - 1s 218us/step - loss: 15917.6177 - val_loss: 15270.5561\n",
      "Epoch 678/1000\n",
      "2304/2304 [==============================] - 0s 181us/step - loss: 16140.7262 - val_loss: 15339.6149\n",
      "Epoch 679/1000\n",
      "2304/2304 [==============================] - 0s 211us/step - loss: 15971.0424 - val_loss: 18203.8803\n",
      "Epoch 680/1000\n",
      "2304/2304 [==============================] - 0s 200us/step - loss: 15938.5333 - val_loss: 16011.8082\n",
      "Epoch 681/1000\n",
      "2304/2304 [==============================] - 1s 223us/step - loss: 15999.8107 - val_loss: 15188.4841\n",
      "Epoch 682/1000\n",
      "2304/2304 [==============================] - 0s 201us/step - loss: 16102.6219 - val_loss: 15677.5152\n",
      "Epoch 683/1000\n",
      "2304/2304 [==============================] - 0s 184us/step - loss: 15867.4827 - val_loss: 15276.7147\n",
      "Epoch 684/1000\n",
      "2304/2304 [==============================] - 0s 170us/step - loss: 15729.5258 - val_loss: 14937.6850\n",
      "Epoch 685/1000\n",
      "2304/2304 [==============================] - 0s 158us/step - loss: 15831.9096 - val_loss: 16971.2890\n",
      "Epoch 686/1000\n",
      "2304/2304 [==============================] - 0s 146us/step - loss: 16541.4131 - val_loss: 15223.9253\n",
      "Epoch 687/1000\n",
      "2304/2304 [==============================] - 0s 170us/step - loss: 16184.2403 - val_loss: 15046.1608\n",
      "Epoch 688/1000\n",
      "2304/2304 [==============================] - 0s 156us/step - loss: 16612.8398 - val_loss: 16944.9735\n",
      "Epoch 689/1000\n",
      "2304/2304 [==============================] - 0s 167us/step - loss: 15976.0877 - val_loss: 15183.1682\n",
      "Epoch 690/1000\n",
      "2304/2304 [==============================] - 0s 180us/step - loss: 15541.1241 - val_loss: 16963.0019\n",
      "Epoch 691/1000\n",
      "2304/2304 [==============================] - 0s 160us/step - loss: 16333.1116 - val_loss: 18156.0048\n",
      "Epoch 692/1000\n",
      "2304/2304 [==============================] - 0s 191us/step - loss: 15826.8525 - val_loss: 15319.4744\n",
      "Epoch 693/1000\n",
      "2304/2304 [==============================] - 0s 178us/step - loss: 15807.9149 - val_loss: 15289.4988\n",
      "Epoch 694/1000\n",
      "2304/2304 [==============================] - 0s 175us/step - loss: 15985.1152 - val_loss: 17103.3360\n",
      "Epoch 695/1000\n",
      "2304/2304 [==============================] - 0s 189us/step - loss: 15947.6561 - val_loss: 14820.9049\n",
      "Epoch 696/1000\n",
      "2304/2304 [==============================] - 0s 179us/step - loss: 16093.3174 - val_loss: 14846.7624\n",
      "Epoch 697/1000\n",
      "2304/2304 [==============================] - 0s 187us/step - loss: 15752.6203 - val_loss: 14892.9498\n",
      "Epoch 698/1000\n",
      "2304/2304 [==============================] - 0s 178us/step - loss: 16348.7305 - val_loss: 16251.1164\n",
      "Epoch 699/1000\n",
      "2304/2304 [==============================] - 0s 203us/step - loss: 15896.6412 - val_loss: 16710.0348\n",
      "Epoch 700/1000\n",
      "2304/2304 [==============================] - 0s 207us/step - loss: 16284.8050 - val_loss: 14999.2298\n",
      "Epoch 701/1000\n",
      "2304/2304 [==============================] - 0s 195us/step - loss: 16700.1643 - val_loss: 18749.3005\n",
      "Epoch 702/1000\n",
      "2304/2304 [==============================] - 0s 211us/step - loss: 15771.6253 - val_loss: 15298.7994\n",
      "Epoch 703/1000\n",
      "2304/2304 [==============================] - 0s 164us/step - loss: 15637.6595 - val_loss: 15332.9573\n",
      "Epoch 704/1000\n",
      "2304/2304 [==============================] - 0s 187us/step - loss: 15782.3876 - val_loss: 15484.1143\n",
      "Epoch 705/1000\n",
      "2304/2304 [==============================] - 0s 188us/step - loss: 16045.9288 - val_loss: 14768.2651\n",
      "Epoch 706/1000\n",
      "2304/2304 [==============================] - 0s 184us/step - loss: 15434.4421 - val_loss: 15168.2432\n",
      "Epoch 707/1000\n",
      "2304/2304 [==============================] - 1s 224us/step - loss: 15874.7776 - val_loss: 14958.5900\n",
      "Epoch 708/1000\n",
      "2304/2304 [==============================] - 0s 210us/step - loss: 16183.8735 - val_loss: 15960.9149\n",
      "Epoch 709/1000\n",
      "2304/2304 [==============================] - 1s 218us/step - loss: 15822.9556 - val_loss: 17814.9905\n",
      "Epoch 710/1000\n",
      "2304/2304 [==============================] - 0s 215us/step - loss: 15479.1916 - val_loss: 14978.8392\n",
      "Epoch 711/1000\n",
      "2304/2304 [==============================] - 1s 217us/step - loss: 15848.4543 - val_loss: 15871.9469\n",
      "Epoch 712/1000\n",
      "2304/2304 [==============================] - 0s 182us/step - loss: 15603.2021 - val_loss: 16840.6388\n",
      "Epoch 713/1000\n",
      "2304/2304 [==============================] - 1s 237us/step - loss: 15475.3397 - val_loss: 14962.5663\n",
      "Epoch 714/1000\n",
      "2304/2304 [==============================] - 0s 209us/step - loss: 15774.1079 - val_loss: 15486.3407\n",
      "Epoch 715/1000\n",
      "2304/2304 [==============================] - 0s 174us/step - loss: 15404.4844 - val_loss: 16058.0693\n",
      "Epoch 716/1000\n",
      "2304/2304 [==============================] - 0s 192us/step - loss: 15520.4504 - val_loss: 15162.7443\n",
      "Epoch 717/1000\n",
      "2304/2304 [==============================] - 0s 162us/step - loss: 15353.1824 - val_loss: 16031.4629\n",
      "Epoch 718/1000\n",
      "2304/2304 [==============================] - 1s 237us/step - loss: 15923.7745 - val_loss: 16597.6352\n",
      "Epoch 719/1000\n",
      "2304/2304 [==============================] - 1s 227us/step - loss: 15727.0996 - val_loss: 16544.5378\n",
      "Epoch 720/1000\n",
      "2304/2304 [==============================] - 0s 202us/step - loss: 16021.0448 - val_loss: 15349.3411\n",
      "Epoch 721/1000\n",
      "2304/2304 [==============================] - 0s 188us/step - loss: 16204.7027 - val_loss: 18955.9992\n",
      "Epoch 722/1000\n",
      "2304/2304 [==============================] - 0s 180us/step - loss: 15962.1105 - val_loss: 15279.9665\n",
      "Epoch 723/1000\n",
      "2304/2304 [==============================] - 0s 212us/step - loss: 16001.3698 - val_loss: 15390.4065\n",
      "Epoch 724/1000\n",
      "2304/2304 [==============================] - 0s 186us/step - loss: 15566.4101 - val_loss: 15058.6646\n",
      "Epoch 725/1000\n",
      "2304/2304 [==============================] - 0s 206us/step - loss: 15371.5340 - val_loss: 15848.0725\n",
      "Epoch 726/1000\n",
      "2304/2304 [==============================] - 0s 167us/step - loss: 15902.7498 - val_loss: 15669.7341\n",
      "Epoch 727/1000\n",
      "2304/2304 [==============================] - 0s 183us/step - loss: 15918.2387 - val_loss: 17332.4115\n",
      "Epoch 728/1000\n",
      "2304/2304 [==============================] - 0s 197us/step - loss: 15675.8290 - val_loss: 15063.4837\n",
      "Epoch 729/1000\n",
      "2304/2304 [==============================] - 0s 165us/step - loss: 16504.8120 - val_loss: 15128.0108\n",
      "Epoch 730/1000\n",
      "2304/2304 [==============================] - 0s 206us/step - loss: 16358.6138 - val_loss: 16559.9199\n",
      "Epoch 731/1000\n",
      "2304/2304 [==============================] - 0s 181us/step - loss: 15678.4209 - val_loss: 15459.8240\n",
      "Epoch 732/1000\n",
      "2304/2304 [==============================] - 0s 184us/step - loss: 15664.0483 - val_loss: 16367.6872\n",
      "Epoch 733/1000\n",
      "2304/2304 [==============================] - 0s 203us/step - loss: 15550.1306 - val_loss: 14955.7672\n",
      "Epoch 734/1000\n",
      "2304/2304 [==============================] - 0s 163us/step - loss: 15517.3818 - val_loss: 15733.1702\n",
      "Epoch 735/1000\n",
      "2304/2304 [==============================] - 0s 184us/step - loss: 15784.1819 - val_loss: 16712.5743\n",
      "Epoch 736/1000\n",
      "2304/2304 [==============================] - 0s 165us/step - loss: 15685.2584 - val_loss: 17910.7320\n",
      "Epoch 737/1000\n",
      "2304/2304 [==============================] - 0s 160us/step - loss: 15731.7356 - val_loss: 14977.3759\n",
      "Epoch 738/1000\n",
      "2304/2304 [==============================] - 0s 184us/step - loss: 15430.2680 - val_loss: 15085.3068\n",
      "Epoch 739/1000\n",
      "2304/2304 [==============================] - 0s 178us/step - loss: 15568.8045 - val_loss: 15452.7328\n",
      "Epoch 740/1000\n",
      "2304/2304 [==============================] - 0s 190us/step - loss: 15596.8020 - val_loss: 15242.2955\n",
      "Epoch 741/1000\n",
      "2304/2304 [==============================] - 0s 185us/step - loss: 15759.2838 - val_loss: 16810.7611\n",
      "Epoch 742/1000\n",
      "2304/2304 [==============================] - 0s 172us/step - loss: 15694.9674 - val_loss: 15581.7518\n",
      "Epoch 743/1000\n",
      "2304/2304 [==============================] - 1s 230us/step - loss: 15385.8176 - val_loss: 16264.8839\n",
      "Epoch 744/1000\n",
      "2304/2304 [==============================] - 0s 177us/step - loss: 16002.9347 - val_loss: 15175.7248\n",
      "Epoch 745/1000\n",
      "2304/2304 [==============================] - 1s 220us/step - loss: 15701.7021 - val_loss: 27328.4224\n",
      "Epoch 746/1000\n",
      "2304/2304 [==============================] - 0s 190us/step - loss: 16755.2847 - val_loss: 16561.0156\n",
      "Epoch 747/1000\n",
      "2304/2304 [==============================] - 0s 206us/step - loss: 15705.3693 - val_loss: 15157.7025\n",
      "Epoch 748/1000\n",
      "2304/2304 [==============================] - 0s 211us/step - loss: 15992.5293 - val_loss: 16010.0741\n",
      "Epoch 749/1000\n",
      "2304/2304 [==============================] - 0s 181us/step - loss: 15781.4509 - val_loss: 16874.8112\n",
      "Epoch 750/1000\n",
      "2304/2304 [==============================] - 0s 151us/step - loss: 15497.7762 - val_loss: 14882.0410\n",
      "Epoch 751/1000\n",
      "2304/2304 [==============================] - 0s 164us/step - loss: 15580.1410 - val_loss: 15551.8862\n",
      "Epoch 752/1000\n",
      "2304/2304 [==============================] - 0s 180us/step - loss: 15859.3812 - val_loss: 15853.5249\n",
      "Epoch 753/1000\n",
      "2304/2304 [==============================] - 0s 200us/step - loss: 16086.7557 - val_loss: 19256.7433\n",
      "Epoch 754/1000\n",
      "2304/2304 [==============================] - 0s 169us/step - loss: 15550.8333 - val_loss: 15359.7258\n",
      "Epoch 755/1000\n",
      "2304/2304 [==============================] - 0s 197us/step - loss: 15328.5786 - val_loss: 16440.9122\n",
      "Epoch 756/1000\n",
      "2304/2304 [==============================] - 0s 190us/step - loss: 15392.1177 - val_loss: 15762.1488\n",
      "Epoch 757/1000\n",
      "2304/2304 [==============================] - 0s 184us/step - loss: 15785.4682 - val_loss: 15494.1906\n",
      "Epoch 758/1000\n",
      "2304/2304 [==============================] - 0s 166us/step - loss: 15703.0210 - val_loss: 15509.3846\n",
      "Epoch 759/1000\n",
      "2304/2304 [==============================] - 0s 160us/step - loss: 15331.0922 - val_loss: 15802.1607\n",
      "Epoch 760/1000\n",
      "2304/2304 [==============================] - 0s 173us/step - loss: 15247.6082 - val_loss: 16411.4039\n",
      "Epoch 761/1000\n",
      "2304/2304 [==============================] - 0s 164us/step - loss: 16039.8661 - val_loss: 15363.4384\n",
      "Epoch 762/1000\n",
      "2304/2304 [==============================] - 0s 154us/step - loss: 15748.9177 - val_loss: 15519.9504\n",
      "Epoch 763/1000\n",
      "2304/2304 [==============================] - 0s 160us/step - loss: 15681.3590 - val_loss: 18876.0835\n",
      "Epoch 764/1000\n",
      "2304/2304 [==============================] - 0s 177us/step - loss: 15887.2979 - val_loss: 15125.0091\n",
      "Epoch 765/1000\n",
      "2304/2304 [==============================] - 0s 170us/step - loss: 15896.0060 - val_loss: 15391.0830\n",
      "Epoch 766/1000\n",
      "2304/2304 [==============================] - 0s 167us/step - loss: 16452.1725 - val_loss: 15562.5811\n",
      "Epoch 767/1000\n",
      "2304/2304 [==============================] - 0s 168us/step - loss: 15381.1195 - val_loss: 15431.8245\n",
      "Epoch 768/1000\n",
      "2304/2304 [==============================] - 0s 165us/step - loss: 15613.2233 - val_loss: 15412.2971\n",
      "Epoch 769/1000\n",
      "2304/2304 [==============================] - 0s 184us/step - loss: 15708.4355 - val_loss: 17505.3505\n",
      "Epoch 770/1000\n",
      "2304/2304 [==============================] - 0s 175us/step - loss: 15301.4404 - val_loss: 24031.4952\n",
      "Epoch 771/1000\n",
      "2304/2304 [==============================] - 0s 195us/step - loss: 15952.3724 - val_loss: 15322.4624\n",
      "Epoch 772/1000\n",
      "2304/2304 [==============================] - 0s 184us/step - loss: 15533.9097 - val_loss: 15087.2802\n",
      "Epoch 773/1000\n",
      "2304/2304 [==============================] - 0s 191us/step - loss: 15585.4231 - val_loss: 15302.7652\n",
      "Epoch 774/1000\n",
      "2304/2304 [==============================] - 0s 215us/step - loss: 15575.7275 - val_loss: 16471.5999\n",
      "Epoch 775/1000\n",
      "2304/2304 [==============================] - 0s 163us/step - loss: 15576.2944 - val_loss: 16543.0087\n",
      "Epoch 776/1000\n",
      "2304/2304 [==============================] - 0s 154us/step - loss: 15796.2708 - val_loss: 16200.5290\n",
      "Epoch 777/1000\n",
      "2304/2304 [==============================] - 0s 168us/step - loss: 15055.8997 - val_loss: 15248.1605\n",
      "Epoch 778/1000\n",
      "2304/2304 [==============================] - 0s 171us/step - loss: 15812.4455 - val_loss: 15613.5623\n",
      "Epoch 779/1000\n",
      "2304/2304 [==============================] - 0s 186us/step - loss: 14892.6606 - val_loss: 16151.6577\n",
      "Epoch 780/1000\n",
      "2304/2304 [==============================] - 0s 181us/step - loss: 15322.2764 - val_loss: 15474.9913\n",
      "Epoch 781/1000\n",
      "2304/2304 [==============================] - 1s 217us/step - loss: 15573.2356 - val_loss: 16195.0133\n",
      "Epoch 782/1000\n",
      "2304/2304 [==============================] - 0s 203us/step - loss: 15689.5718 - val_loss: 15834.7591\n",
      "Epoch 783/1000\n",
      "2304/2304 [==============================] - 1s 223us/step - loss: 15285.4572 - val_loss: 15798.1143\n",
      "Epoch 784/1000\n",
      "2304/2304 [==============================] - 1s 217us/step - loss: 15885.6951 - val_loss: 15487.8396\n",
      "Epoch 785/1000\n",
      "2304/2304 [==============================] - 1s 221us/step - loss: 15545.7298 - val_loss: 17016.6560\n",
      "Epoch 786/1000\n",
      "2304/2304 [==============================] - 1s 226us/step - loss: 15718.3690 - val_loss: 17010.7149\n",
      "Epoch 787/1000\n",
      "2304/2304 [==============================] - 0s 213us/step - loss: 15748.0126 - val_loss: 16175.5357\n",
      "Epoch 788/1000\n",
      "2304/2304 [==============================] - 1s 230us/step - loss: 15276.4818 - val_loss: 17199.3104\n",
      "Epoch 789/1000\n",
      "2304/2304 [==============================] - 0s 188us/step - loss: 15815.0408 - val_loss: 15753.2700\n",
      "Epoch 790/1000\n",
      "2304/2304 [==============================] - 0s 209us/step - loss: 15477.1786 - val_loss: 16384.1579\n",
      "Epoch 791/1000\n",
      "2304/2304 [==============================] - 0s 176us/step - loss: 15314.5823 - val_loss: 17073.7785\n",
      "Epoch 792/1000\n",
      "2304/2304 [==============================] - 0s 199us/step - loss: 15195.3716 - val_loss: 16502.3516\n",
      "Epoch 793/1000\n",
      "2304/2304 [==============================] - 0s 187us/step - loss: 15464.9308 - val_loss: 15961.7189\n",
      "Epoch 794/1000\n",
      "2304/2304 [==============================] - 0s 197us/step - loss: 15363.3995 - val_loss: 21434.7328\n",
      "Epoch 795/1000\n",
      "2304/2304 [==============================] - 0s 169us/step - loss: 15146.8473 - val_loss: 15974.8062\n",
      "Epoch 796/1000\n",
      "2304/2304 [==============================] - 0s 158us/step - loss: 16139.6265 - val_loss: 17357.3716\n",
      "Epoch 797/1000\n",
      "2304/2304 [==============================] - 0s 187us/step - loss: 15524.1360 - val_loss: 19597.7740\n",
      "Epoch 798/1000\n",
      "2304/2304 [==============================] - 0s 201us/step - loss: 16095.9691 - val_loss: 17427.7463\n",
      "Epoch 799/1000\n",
      "2304/2304 [==============================] - 0s 213us/step - loss: 15881.8823 - val_loss: 15471.6372\n",
      "Epoch 800/1000\n",
      "2304/2304 [==============================] - 1s 220us/step - loss: 15434.2859 - val_loss: 19196.1585\n",
      "Epoch 801/1000\n",
      "2304/2304 [==============================] - 0s 204us/step - loss: 15747.7641 - val_loss: 16144.5428\n",
      "Epoch 802/1000\n",
      "2304/2304 [==============================] - 1s 240us/step - loss: 15002.3702 - val_loss: 16327.7195\n",
      "Epoch 803/1000\n",
      "2304/2304 [==============================] - 0s 190us/step - loss: 15340.7066 - val_loss: 17817.6039\n",
      "Epoch 804/1000\n",
      "2304/2304 [==============================] - 0s 203us/step - loss: 15441.5999 - val_loss: 16959.2431\n",
      "Epoch 805/1000\n",
      "2304/2304 [==============================] - 0s 191us/step - loss: 15535.4066 - val_loss: 17030.3697\n",
      "Epoch 806/1000\n",
      "2304/2304 [==============================] - 0s 211us/step - loss: 15638.1463 - val_loss: 17063.6924\n",
      "Epoch 807/1000\n",
      "2304/2304 [==============================] - 0s 198us/step - loss: 15307.8243 - val_loss: 16285.1160\n",
      "Epoch 808/1000\n",
      "2304/2304 [==============================] - 0s 208us/step - loss: 15848.6765 - val_loss: 18633.4368\n",
      "Epoch 809/1000\n",
      "2304/2304 [==============================] - 0s 208us/step - loss: 15317.9974 - val_loss: 16280.4626\n",
      "Epoch 810/1000\n",
      "2304/2304 [==============================] - 0s 196us/step - loss: 15248.2664 - val_loss: 15938.3633\n",
      "Epoch 811/1000\n",
      "2304/2304 [==============================] - 1s 226us/step - loss: 15500.2373 - val_loss: 16113.4201\n",
      "Epoch 812/1000\n",
      "2304/2304 [==============================] - 0s 193us/step - loss: 15311.9537 - val_loss: 15626.6745\n",
      "Epoch 813/1000\n",
      "2304/2304 [==============================] - 0s 216us/step - loss: 14955.7584 - val_loss: 17665.5274\n",
      "Epoch 814/1000\n",
      "2304/2304 [==============================] - 0s 187us/step - loss: 15600.2130 - val_loss: 15680.9061\n",
      "Epoch 815/1000\n",
      "2304/2304 [==============================] - 0s 213us/step - loss: 15266.7254 - val_loss: 16870.0728\n",
      "Epoch 816/1000\n",
      "2304/2304 [==============================] - 0s 202us/step - loss: 15692.0124 - val_loss: 16447.6542\n",
      "Epoch 817/1000\n",
      "2304/2304 [==============================] - 0s 205us/step - loss: 15748.5941 - val_loss: 19178.6388\n",
      "Epoch 818/1000\n",
      "2304/2304 [==============================] - 1s 221us/step - loss: 15712.7866 - val_loss: 17555.1623\n",
      "Epoch 819/1000\n",
      "2304/2304 [==============================] - 0s 207us/step - loss: 15319.5264 - val_loss: 15274.9733\n",
      "Epoch 820/1000\n",
      "2304/2304 [==============================] - 1s 223us/step - loss: 15510.9551 - val_loss: 16748.9528\n",
      "Epoch 821/1000\n",
      "2304/2304 [==============================] - 0s 191us/step - loss: 15954.9670 - val_loss: 16422.6125\n",
      "Epoch 822/1000\n",
      "2304/2304 [==============================] - 0s 207us/step - loss: 15262.0432 - val_loss: 17213.5764\n",
      "Epoch 823/1000\n",
      "2304/2304 [==============================] - 0s 191us/step - loss: 15395.5709 - val_loss: 18593.7565\n",
      "Epoch 824/1000\n",
      "2304/2304 [==============================] - 0s 204us/step - loss: 15590.3919 - val_loss: 15037.5942\n",
      "Epoch 825/1000\n",
      "2304/2304 [==============================] - 0s 201us/step - loss: 15402.7848 - val_loss: 15556.7659\n",
      "Epoch 826/1000\n",
      "2304/2304 [==============================] - 0s 209us/step - loss: 15457.6529 - val_loss: 16364.2342\n",
      "Epoch 827/1000\n",
      "2304/2304 [==============================] - 0s 190us/step - loss: 16203.8386 - val_loss: 18126.9440\n",
      "Epoch 828/1000\n",
      "2304/2304 [==============================] - 0s 185us/step - loss: 15850.9883 - val_loss: 16605.3270\n",
      "Epoch 829/1000\n",
      "2304/2304 [==============================] - 0s 158us/step - loss: 15459.2634 - val_loss: 15625.3130\n",
      "Epoch 830/1000\n",
      "2304/2304 [==============================] - 0s 157us/step - loss: 15401.5080 - val_loss: 15640.5685\n",
      "Epoch 831/1000\n",
      "2304/2304 [==============================] - 0s 153us/step - loss: 15045.1735 - val_loss: 18568.4363\n",
      "Epoch 832/1000\n",
      "2304/2304 [==============================] - 0s 172us/step - loss: 15384.1463 - val_loss: 15540.5697\n",
      "Epoch 833/1000\n",
      "2304/2304 [==============================] - 0s 177us/step - loss: 14999.9753 - val_loss: 15823.1397\n",
      "Epoch 834/1000\n",
      "2304/2304 [==============================] - 0s 178us/step - loss: 15189.7235 - val_loss: 21045.2030\n",
      "Epoch 835/1000\n",
      "2304/2304 [==============================] - 0s 188us/step - loss: 15690.2415 - val_loss: 16034.4492\n",
      "Epoch 836/1000\n",
      "2304/2304 [==============================] - 0s 161us/step - loss: 15059.8036 - val_loss: 16398.5446\n",
      "Epoch 837/1000\n",
      "2304/2304 [==============================] - 0s 166us/step - loss: 15049.2651 - val_loss: 16240.8731\n",
      "Epoch 838/1000\n",
      "2304/2304 [==============================] - 0s 181us/step - loss: 15179.7926 - val_loss: 16689.4264\n",
      "Epoch 839/1000\n",
      "2304/2304 [==============================] - 0s 183us/step - loss: 15832.4880 - val_loss: 17099.7877\n",
      "Epoch 840/1000\n",
      "2304/2304 [==============================] - 0s 187us/step - loss: 15134.2337 - val_loss: 15712.3178\n",
      "Epoch 841/1000\n",
      "2304/2304 [==============================] - 0s 165us/step - loss: 14997.5975 - val_loss: 16187.1505\n",
      "Epoch 842/1000\n",
      "2304/2304 [==============================] - 0s 203us/step - loss: 15622.0241 - val_loss: 16431.5476\n",
      "Epoch 843/1000\n",
      "2304/2304 [==============================] - 0s 201us/step - loss: 14994.5026 - val_loss: 16022.1950\n",
      "Epoch 844/1000\n",
      "2304/2304 [==============================] - 0s 203us/step - loss: 15396.3160 - val_loss: 15862.2470\n",
      "Epoch 845/1000\n",
      "2304/2304 [==============================] - 0s 205us/step - loss: 15044.2166 - val_loss: 15827.8636\n",
      "Epoch 846/1000\n",
      "2304/2304 [==============================] - 0s 209us/step - loss: 15047.3341 - val_loss: 15550.2389\n",
      "Epoch 847/1000\n",
      "2304/2304 [==============================] - 1s 247us/step - loss: 15364.4760 - val_loss: 15169.1490\n",
      "Epoch 848/1000\n",
      "2304/2304 [==============================] - 0s 181us/step - loss: 14935.3493 - val_loss: 15528.2770\n",
      "Epoch 849/1000\n",
      "2304/2304 [==============================] - 0s 179us/step - loss: 16128.4220 - val_loss: 16866.9788\n",
      "Epoch 850/1000\n",
      "2304/2304 [==============================] - 0s 176us/step - loss: 14998.4810 - val_loss: 15514.7151\n",
      "Epoch 851/1000\n",
      "2304/2304 [==============================] - 0s 163us/step - loss: 14838.7378 - val_loss: 17906.0572\n",
      "Epoch 852/1000\n",
      "2304/2304 [==============================] - 0s 153us/step - loss: 15469.9900 - val_loss: 18383.5345\n",
      "Epoch 853/1000\n",
      "2304/2304 [==============================] - 0s 166us/step - loss: 15526.3865 - val_loss: 19501.8478\n",
      "Epoch 854/1000\n",
      "2304/2304 [==============================] - 0s 207us/step - loss: 15504.6436 - val_loss: 15643.9340\n",
      "Epoch 855/1000\n",
      "2304/2304 [==============================] - 0s 206us/step - loss: 15710.7633 - val_loss: 16162.7306\n",
      "Epoch 856/1000\n",
      "2304/2304 [==============================] - 0s 214us/step - loss: 15092.4516 - val_loss: 15798.9766\n",
      "Epoch 857/1000\n",
      "2304/2304 [==============================] - 1s 222us/step - loss: 15586.1242 - val_loss: 16014.1392\n",
      "Epoch 858/1000\n",
      "2304/2304 [==============================] - 0s 193us/step - loss: 15323.1133 - val_loss: 17225.1693\n",
      "Epoch 859/1000\n",
      "2304/2304 [==============================] - 0s 216us/step - loss: 15221.4109 - val_loss: 15486.9944\n",
      "Epoch 860/1000\n",
      "2304/2304 [==============================] - 0s 190us/step - loss: 15373.1023 - val_loss: 15845.7509\n",
      "Epoch 861/1000\n",
      "2304/2304 [==============================] - 0s 213us/step - loss: 15956.4382 - val_loss: 16985.0971\n",
      "Epoch 862/1000\n",
      "2304/2304 [==============================] - 0s 185us/step - loss: 15674.7572 - val_loss: 15941.2755\n",
      "Epoch 863/1000\n",
      "2304/2304 [==============================] - 1s 221us/step - loss: 15559.6896 - val_loss: 16087.9794\n",
      "Epoch 864/1000\n",
      "2304/2304 [==============================] - 0s 186us/step - loss: 15343.3002 - val_loss: 17092.3546\n",
      "Epoch 865/1000\n",
      "2304/2304 [==============================] - 0s 183us/step - loss: 15024.1012 - val_loss: 16454.9656\n",
      "Epoch 866/1000\n",
      "2304/2304 [==============================] - 0s 138us/step - loss: 15004.7627 - val_loss: 17560.9867\n",
      "Epoch 867/1000\n",
      "2304/2304 [==============================] - 0s 161us/step - loss: 15135.0742 - val_loss: 15805.9495\n",
      "Epoch 868/1000\n",
      "2304/2304 [==============================] - 0s 186us/step - loss: 14976.9695 - val_loss: 15769.8319\n",
      "Epoch 869/1000\n",
      "2304/2304 [==============================] - 0s 208us/step - loss: 16150.1157 - val_loss: 16210.2923\n",
      "Epoch 870/1000\n",
      "2304/2304 [==============================] - 0s 183us/step - loss: 15043.5606 - val_loss: 16052.7206\n",
      "Epoch 871/1000\n",
      "2304/2304 [==============================] - 1s 228us/step - loss: 15163.5203 - val_loss: 16438.3202\n",
      "Epoch 872/1000\n",
      "2304/2304 [==============================] - 0s 191us/step - loss: 15128.7008 - val_loss: 17502.6072\n",
      "Epoch 873/1000\n",
      "2304/2304 [==============================] - 0s 191us/step - loss: 14933.1239 - val_loss: 15614.9388\n",
      "Epoch 874/1000\n",
      "2304/2304 [==============================] - 0s 193us/step - loss: 15738.4858 - val_loss: 16355.9266\n",
      "Epoch 875/1000\n",
      "2304/2304 [==============================] - 0s 205us/step - loss: 14530.8710 - val_loss: 15926.3705\n",
      "Epoch 876/1000\n",
      "2304/2304 [==============================] - 1s 228us/step - loss: 15965.7749 - val_loss: 16884.9617\n",
      "Epoch 877/1000\n",
      "2304/2304 [==============================] - 0s 189us/step - loss: 15034.6506 - val_loss: 16597.6891\n",
      "Epoch 878/1000\n",
      "2304/2304 [==============================] - 0s 210us/step - loss: 15578.3136 - val_loss: 15864.5970\n",
      "Epoch 879/1000\n",
      "2304/2304 [==============================] - 0s 177us/step - loss: 15304.8737 - val_loss: 16876.1630\n",
      "Epoch 880/1000\n",
      "2304/2304 [==============================] - 0s 193us/step - loss: 14918.7917 - val_loss: 16101.3064\n",
      "Epoch 881/1000\n",
      "2304/2304 [==============================] - 0s 206us/step - loss: 15372.4276 - val_loss: 17078.8214\n",
      "Epoch 882/1000\n",
      "2304/2304 [==============================] - 0s 180us/step - loss: 15111.6816 - val_loss: 16760.5239\n",
      "Epoch 883/1000\n",
      "2304/2304 [==============================] - 0s 175us/step - loss: 14807.8104 - val_loss: 15868.9624\n",
      "Epoch 884/1000\n",
      "2304/2304 [==============================] - 0s 186us/step - loss: 15459.8217 - val_loss: 21439.9417\n",
      "Epoch 885/1000\n",
      "2304/2304 [==============================] - 0s 187us/step - loss: 15430.8758 - val_loss: 16235.5773\n",
      "Epoch 886/1000\n",
      "2304/2304 [==============================] - 0s 190us/step - loss: 14742.8949 - val_loss: 16140.6547\n",
      "Epoch 887/1000\n",
      "2304/2304 [==============================] - 0s 178us/step - loss: 15195.1661 - val_loss: 16421.4928\n",
      "Epoch 888/1000\n",
      "2304/2304 [==============================] - 0s 178us/step - loss: 14886.2702 - val_loss: 17011.6886\n",
      "Epoch 889/1000\n",
      "2304/2304 [==============================] - 0s 188us/step - loss: 15170.0946 - val_loss: 15802.1965\n",
      "Epoch 890/1000\n",
      "2304/2304 [==============================] - 0s 186us/step - loss: 15546.5308 - val_loss: 15680.5302\n",
      "Epoch 891/1000\n",
      "2304/2304 [==============================] - 0s 185us/step - loss: 14818.8351 - val_loss: 15605.0576\n",
      "Epoch 892/1000\n",
      "2304/2304 [==============================] - 0s 180us/step - loss: 15099.8558 - val_loss: 15808.2105\n",
      "Epoch 893/1000\n",
      "2304/2304 [==============================] - 0s 190us/step - loss: 15611.5370 - val_loss: 17170.2276\n",
      "Epoch 894/1000\n",
      "2304/2304 [==============================] - 0s 179us/step - loss: 15710.0504 - val_loss: 15887.4216\n",
      "Epoch 895/1000\n",
      "2304/2304 [==============================] - 0s 152us/step - loss: 15892.1674 - val_loss: 20601.1174\n",
      "Epoch 896/1000\n",
      "2304/2304 [==============================] - 0s 156us/step - loss: 15131.7792 - val_loss: 15595.5647\n",
      "Epoch 897/1000\n",
      "2304/2304 [==============================] - 0s 160us/step - loss: 15390.7451 - val_loss: 17252.4830\n",
      "Epoch 898/1000\n",
      "2304/2304 [==============================] - 0s 165us/step - loss: 15464.2268 - val_loss: 17214.0157\n",
      "Epoch 899/1000\n",
      "2304/2304 [==============================] - 0s 167us/step - loss: 15357.8166 - val_loss: 15818.2398\n",
      "Epoch 900/1000\n",
      "2304/2304 [==============================] - 0s 187us/step - loss: 15358.3241 - val_loss: 16082.7651\n",
      "Epoch 901/1000\n",
      "2304/2304 [==============================] - 0s 208us/step - loss: 15052.5367 - val_loss: 20905.9322\n",
      "Epoch 902/1000\n",
      "2304/2304 [==============================] - 0s 199us/step - loss: 15185.4634 - val_loss: 15641.3598\n",
      "Epoch 903/1000\n",
      "2304/2304 [==============================] - 0s 189us/step - loss: 14950.8050 - val_loss: 15745.6148\n",
      "Epoch 904/1000\n",
      "2304/2304 [==============================] - 0s 188us/step - loss: 15272.5279 - val_loss: 19712.8235\n",
      "Epoch 905/1000\n",
      "2304/2304 [==============================] - 0s 165us/step - loss: 14881.1547 - val_loss: 18249.8226\n",
      "Epoch 906/1000\n",
      "2304/2304 [==============================] - 0s 151us/step - loss: 15389.4073 - val_loss: 15739.0463\n",
      "Epoch 907/1000\n",
      "2304/2304 [==============================] - 0s 153us/step - loss: 15158.1596 - val_loss: 17621.9580\n",
      "Epoch 908/1000\n",
      "2304/2304 [==============================] - 0s 160us/step - loss: 15278.5723 - val_loss: 15806.2577\n",
      "Epoch 909/1000\n",
      "2304/2304 [==============================] - 0s 162us/step - loss: 14881.6410 - val_loss: 15799.0196\n",
      "Epoch 910/1000\n",
      "2304/2304 [==============================] - 0s 158us/step - loss: 14773.8747 - val_loss: 15917.3362\n",
      "Epoch 911/1000\n",
      "2304/2304 [==============================] - 0s 185us/step - loss: 14782.5824 - val_loss: 16692.9778\n",
      "Epoch 912/1000\n",
      "2304/2304 [==============================] - 0s 179us/step - loss: 14988.3576 - val_loss: 18667.9125\n",
      "Epoch 913/1000\n",
      "2304/2304 [==============================] - 0s 170us/step - loss: 15455.6516 - val_loss: 16045.9002\n",
      "Epoch 914/1000\n",
      "2304/2304 [==============================] - 0s 213us/step - loss: 15305.5462 - val_loss: 16345.2297\n",
      "Epoch 915/1000\n",
      "2304/2304 [==============================] - 0s 180us/step - loss: 14712.1247 - val_loss: 16582.2549\n",
      "Epoch 916/1000\n",
      "2304/2304 [==============================] - 0s 187us/step - loss: 14692.4150 - val_loss: 15351.1731\n",
      "Epoch 917/1000\n",
      "2304/2304 [==============================] - 0s 209us/step - loss: 14835.9298 - val_loss: 15724.0304\n",
      "Epoch 918/1000\n",
      "2304/2304 [==============================] - 0s 184us/step - loss: 15045.8141 - val_loss: 16846.0800\n",
      "Epoch 919/1000\n",
      "2304/2304 [==============================] - 0s 191us/step - loss: 14898.6381 - val_loss: 15712.3976\n",
      "Epoch 920/1000\n",
      "2304/2304 [==============================] - 0s 180us/step - loss: 15384.2718 - val_loss: 15937.7956\n",
      "Epoch 921/1000\n",
      "2304/2304 [==============================] - 0s 188us/step - loss: 15239.1287 - val_loss: 15405.7764\n",
      "Epoch 922/1000\n",
      "2304/2304 [==============================] - 0s 174us/step - loss: 14600.8388 - val_loss: 15662.7681\n",
      "Epoch 923/1000\n",
      "2304/2304 [==============================] - 0s 194us/step - loss: 14679.3445 - val_loss: 16333.2709\n",
      "Epoch 924/1000\n",
      "2304/2304 [==============================] - 0s 158us/step - loss: 14717.7856 - val_loss: 16979.2405\n",
      "Epoch 925/1000\n",
      "2304/2304 [==============================] - 0s 188us/step - loss: 15847.9597 - val_loss: 15832.1282\n",
      "Epoch 926/1000\n",
      "2304/2304 [==============================] - 0s 188us/step - loss: 15215.0688 - val_loss: 19010.6178\n",
      "Epoch 927/1000\n",
      "2304/2304 [==============================] - 0s 198us/step - loss: 15281.9501 - val_loss: 16628.0925\n",
      "Epoch 928/1000\n",
      "2304/2304 [==============================] - 0s 198us/step - loss: 14447.4873 - val_loss: 17770.8446\n",
      "Epoch 929/1000\n",
      "2304/2304 [==============================] - 1s 226us/step - loss: 15019.7289 - val_loss: 16227.5230\n",
      "Epoch 930/1000\n",
      "2304/2304 [==============================] - 0s 211us/step - loss: 15762.0042 - val_loss: 16218.4868\n",
      "Epoch 931/1000\n",
      "2304/2304 [==============================] - 1s 237us/step - loss: 15180.2039 - val_loss: 18465.4220\n",
      "Epoch 932/1000\n",
      "2304/2304 [==============================] - 0s 180us/step - loss: 15180.1559 - val_loss: 18311.0719\n",
      "Epoch 933/1000\n",
      "2304/2304 [==============================] - 1s 221us/step - loss: 15142.9458 - val_loss: 17182.8852\n",
      "Epoch 934/1000\n",
      "2304/2304 [==============================] - 0s 192us/step - loss: 15281.3991 - val_loss: 23000.6246\n",
      "Epoch 935/1000\n",
      "2304/2304 [==============================] - 0s 201us/step - loss: 14901.5593 - val_loss: 18956.9590\n",
      "Epoch 936/1000\n",
      "2304/2304 [==============================] - 0s 215us/step - loss: 15290.0006 - val_loss: 15928.5469\n",
      "Epoch 937/1000\n",
      "2304/2304 [==============================] - 0s 204us/step - loss: 14862.5747 - val_loss: 16046.0872\n",
      "Epoch 938/1000\n",
      "2304/2304 [==============================] - 0s 195us/step - loss: 14674.4140 - val_loss: 18395.6002\n",
      "Epoch 939/1000\n",
      "2304/2304 [==============================] - 0s 183us/step - loss: 14883.9850 - val_loss: 15420.7831\n",
      "Epoch 940/1000\n",
      "2304/2304 [==============================] - 0s 155us/step - loss: 14389.3078 - val_loss: 15824.2495\n",
      "Epoch 941/1000\n",
      "2304/2304 [==============================] - 0s 155us/step - loss: 14992.1293 - val_loss: 16240.9233\n",
      "Epoch 942/1000\n",
      "2304/2304 [==============================] - 0s 165us/step - loss: 14855.0326 - val_loss: 17275.1708\n",
      "Epoch 943/1000\n",
      "2304/2304 [==============================] - 0s 180us/step - loss: 14923.3633 - val_loss: 15888.2518\n",
      "Epoch 944/1000\n",
      "2304/2304 [==============================] - 0s 168us/step - loss: 14853.8292 - val_loss: 16168.2501\n",
      "Epoch 945/1000\n",
      "2304/2304 [==============================] - 0s 186us/step - loss: 15747.5530 - val_loss: 16248.5999\n",
      "Epoch 946/1000\n",
      "2304/2304 [==============================] - 0s 183us/step - loss: 15471.5925 - val_loss: 15994.0872\n",
      "Epoch 947/1000\n",
      "2304/2304 [==============================] - 0s 169us/step - loss: 14722.9360 - val_loss: 19153.2886\n",
      "Epoch 948/1000\n",
      "2304/2304 [==============================] - 0s 186us/step - loss: 14769.1282 - val_loss: 15763.9030\n",
      "Epoch 949/1000\n",
      "2304/2304 [==============================] - 0s 180us/step - loss: 14775.4580 - val_loss: 16988.1168\n",
      "Epoch 950/1000\n",
      "2304/2304 [==============================] - 0s 193us/step - loss: 14805.8066 - val_loss: 17261.6596\n",
      "Epoch 951/1000\n",
      "2304/2304 [==============================] - 0s 197us/step - loss: 15244.9731 - val_loss: 15974.8842\n",
      "Epoch 952/1000\n",
      "2304/2304 [==============================] - 0s 158us/step - loss: 14478.0861 - val_loss: 16780.8885\n",
      "Epoch 953/1000\n",
      "2304/2304 [==============================] - 0s 200us/step - loss: 14905.7119 - val_loss: 16703.5300\n",
      "Epoch 954/1000\n",
      "2304/2304 [==============================] - 0s 199us/step - loss: 14898.7343 - val_loss: 16115.9644\n",
      "Epoch 955/1000\n",
      "2304/2304 [==============================] - 0s 191us/step - loss: 14989.5275 - val_loss: 17432.3197\n",
      "Epoch 956/1000\n",
      "2304/2304 [==============================] - 0s 199us/step - loss: 15171.6411 - val_loss: 16275.4740\n",
      "Epoch 957/1000\n",
      "2304/2304 [==============================] - 0s 166us/step - loss: 15056.2688 - val_loss: 15869.4300\n",
      "Epoch 958/1000\n",
      "2304/2304 [==============================] - 0s 208us/step - loss: 14653.3553 - val_loss: 16524.6161\n",
      "Epoch 959/1000\n",
      "2304/2304 [==============================] - 0s 177us/step - loss: 14789.6282 - val_loss: 17729.1868\n",
      "Epoch 960/1000\n",
      "2304/2304 [==============================] - 0s 181us/step - loss: 14922.2982 - val_loss: 16182.4603\n",
      "Epoch 961/1000\n",
      "2304/2304 [==============================] - 0s 189us/step - loss: 15057.2726 - val_loss: 15812.1052\n",
      "Epoch 962/1000\n",
      "2304/2304 [==============================] - 0s 167us/step - loss: 15072.3480 - val_loss: 16380.2653\n",
      "Epoch 963/1000\n",
      "2304/2304 [==============================] - 0s 186us/step - loss: 15232.5061 - val_loss: 15583.2414\n",
      "Epoch 964/1000\n",
      "2304/2304 [==============================] - 0s 187us/step - loss: 14322.2373 - val_loss: 15320.3145\n",
      "Epoch 965/1000\n",
      "2304/2304 [==============================] - 0s 207us/step - loss: 14646.7511 - val_loss: 15457.3592\n",
      "Epoch 966/1000\n",
      "2304/2304 [==============================] - 0s 196us/step - loss: 15002.2910 - val_loss: 18130.5802\n",
      "Epoch 967/1000\n",
      "2304/2304 [==============================] - 0s 165us/step - loss: 14819.5776 - val_loss: 15699.3722\n",
      "Epoch 968/1000\n",
      "2304/2304 [==============================] - 0s 181us/step - loss: 14574.1234 - val_loss: 17760.0061\n",
      "Epoch 969/1000\n",
      "2304/2304 [==============================] - 0s 182us/step - loss: 14759.8245 - val_loss: 15684.8602\n",
      "Epoch 970/1000\n",
      "2304/2304 [==============================] - 0s 188us/step - loss: 15492.2761 - val_loss: 17056.8939\n",
      "Epoch 971/1000\n",
      "2304/2304 [==============================] - 0s 185us/step - loss: 15017.3222 - val_loss: 15663.7973\n",
      "Epoch 972/1000\n",
      "2304/2304 [==============================] - 0s 182us/step - loss: 14603.3661 - val_loss: 17149.9502\n",
      "Epoch 973/1000\n",
      "2304/2304 [==============================] - 0s 194us/step - loss: 15227.5361 - val_loss: 15477.1448\n",
      "Epoch 974/1000\n",
      "2304/2304 [==============================] - 0s 183us/step - loss: 14531.7996 - val_loss: 16927.0179\n",
      "Epoch 975/1000\n",
      "2304/2304 [==============================] - 0s 201us/step - loss: 14561.2064 - val_loss: 15988.2125\n",
      "Epoch 976/1000\n",
      "2304/2304 [==============================] - 0s 191us/step - loss: 14624.9567 - val_loss: 15565.9879\n",
      "Epoch 977/1000\n",
      "2304/2304 [==============================] - 0s 184us/step - loss: 15330.7156 - val_loss: 15729.7421\n",
      "Epoch 978/1000\n",
      "2304/2304 [==============================] - 0s 159us/step - loss: 14342.5426 - val_loss: 17942.3898\n",
      "Epoch 979/1000\n",
      "2304/2304 [==============================] - 0s 178us/step - loss: 15160.7547 - val_loss: 16183.7220\n",
      "Epoch 980/1000\n",
      "2304/2304 [==============================] - 0s 151us/step - loss: 14469.3693 - val_loss: 15855.4862\n",
      "Epoch 981/1000\n",
      "2304/2304 [==============================] - 0s 171us/step - loss: 14767.8689 - val_loss: 16133.8662\n",
      "Epoch 982/1000\n",
      "2304/2304 [==============================] - 0s 164us/step - loss: 14462.7738 - val_loss: 15776.2371\n",
      "Epoch 983/1000\n",
      "2304/2304 [==============================] - 0s 151us/step - loss: 14679.6093 - val_loss: 15829.5507\n",
      "Epoch 984/1000\n",
      "2304/2304 [==============================] - 0s 165us/step - loss: 14251.4854 - val_loss: 15936.5524\n",
      "Epoch 985/1000\n",
      "2304/2304 [==============================] - 0s 153us/step - loss: 14499.8176 - val_loss: 17774.8626\n",
      "Epoch 986/1000\n",
      "2304/2304 [==============================] - 0s 172us/step - loss: 15033.9638 - val_loss: 15901.6477\n",
      "Epoch 987/1000\n",
      "2304/2304 [==============================] - 0s 187us/step - loss: 14635.1225 - val_loss: 16234.4269\n",
      "Epoch 988/1000\n",
      "2304/2304 [==============================] - 0s 193us/step - loss: 15019.8858 - val_loss: 18144.0585\n",
      "Epoch 989/1000\n",
      "2304/2304 [==============================] - 1s 230us/step - loss: 14762.4085 - val_loss: 16474.0811\n",
      "Epoch 990/1000\n",
      "2304/2304 [==============================] - 0s 211us/step - loss: 14801.4368 - val_loss: 16410.6220\n",
      "Epoch 991/1000\n",
      "2304/2304 [==============================] - 1s 232us/step - loss: 15099.9366 - val_loss: 17558.9278\n",
      "Epoch 992/1000\n",
      "2304/2304 [==============================] - 0s 194us/step - loss: 14617.9813 - val_loss: 15522.7871\n",
      "Epoch 993/1000\n",
      "2304/2304 [==============================] - 1s 224us/step - loss: 14863.4639 - val_loss: 15960.9805\n",
      "Epoch 994/1000\n",
      "2304/2304 [==============================] - 0s 199us/step - loss: 14912.8320 - val_loss: 15806.0241\n",
      "Epoch 995/1000\n",
      "2304/2304 [==============================] - 1s 222us/step - loss: 14538.6486 - val_loss: 15850.9804\n",
      "Epoch 996/1000\n",
      "2304/2304 [==============================] - 1s 217us/step - loss: 14775.6643 - val_loss: 15534.5641\n",
      "Epoch 997/1000\n",
      "2304/2304 [==============================] - 1s 240us/step - loss: 14319.1574 - val_loss: 18252.4967\n",
      "Epoch 998/1000\n",
      "2304/2304 [==============================] - 0s 204us/step - loss: 15195.4246 - val_loss: 15606.9675\n",
      "Epoch 999/1000\n",
      "2304/2304 [==============================] - 0s 211us/step - loss: 14978.5628 - val_loss: 16652.2092\n",
      "Epoch 1000/1000\n",
      "2304/2304 [==============================] - 0s 213us/step - loss: 14910.5368 - val_loss: 15811.1220\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU,PReLU,ELU\n",
    "from keras.layers import Dropout\n",
    "\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 50, init = 'he_uniform',activation='relu',input_dim = 174))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 25, init = 'he_uniform',activation='relu'))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(output_dim = 50, init = 'he_uniform',activation='relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'he_uniform'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(loss=root_mean_squared_error, optimizer='Adamax')\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "model_history=classifier.fit(X_train.values, y_train.values,validation_split=0.20, batch_size = 10, nb_epoch = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_pred=classifier.predict(df_Test.drop(['SalePrice'],axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=pd.DataFrame(ann_pred)\n",
    "sub_df=pd.read_csv('D:/houseRent/sample_submission.csv')\n",
    "datasets=pd.concat([sub_df['Id'],pred],axis=1)\n",
    "datasets.columns=['Id','SalePrice']\n",
    "datasets.to_csv('D:/houseRent/sample_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
